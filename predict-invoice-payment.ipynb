{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhu4Y7YWnjgqqObypqG7SM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oe4itJD5QlwP","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_L2WUirTE9Kp","colab_type":"code","colab":{}},"source":["!pip -q install imbalanced-learn --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jt-ZWeIcE-cu","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYxkDaUCF_mu","colab_type":"code","colab":{}},"source":["def plot_data_distribution(train, test, labels=None):\n","    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 2))\n","    sns.countplot(np.squeeze(train), ax=ax[0]).set_title(f'Train - {col}')\n","    sns.countplot(np.squeeze(test), ax=ax[1]).set_title(f'Test - {col}')\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confuncion Matrix'):\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax).set_title(title)\n","\n","def plot_feature_importance(features, importances, plot=True):\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(x_train.shape[1]):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    if plot:\n","        plt.figure(figsize=(10, 8))\n","        plt.barh(range(x_train.shape[1]), importances[indices])\n","        plt.yticks(range(x_train.shape[1]), features[indices])\n","        plt.title('Feature Importance')\n","        plt.gca().invert_yaxis()\n","        plt.show()\n","\n","def setup_buckets(df, bins, columns):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]}-{bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    labels[0], labels[-1] = 'on-time', labels[-1].replace('-inf', '+')\n","\n","    for col in columns:\n","        df[col + 'Bucket'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","        df[[col + 'BucketCT']] = df[[col + 'Bucket']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return (df, labels)\n","\n","def split_data_month_window(df, col, date, month_window):\n","    date_0 = pd.to_datetime(date)\n","    date_1 = date_0 - pd.DateOffset(months=month_window)\n","    date_2 = date_0 + pd.DateOffset(months=1)\n","    train = df[(df[col] >= date_1) & (df[col] < date_0)]\n","    test = df[(df[col] >= date_0) & (df[col] < date_2)]\n","    return train, test\n","\n","def resample(df, x_column, y_column, func):\n","    dtypes = df[x_column].dtypes.to_dict()\n","    dtypes.update(df[y_column].dtypes.to_dict())\n","\n","    x, y = df[x_column].values, df[y_column].values\n","    x, y = func.fit_resample(x, y)\n","    xy = np.concatenate((x, np.expand_dims(y, axis=1)), axis=1)\n","    \n","    data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","    data = data.astype(dtypes)\n","    return data\n","\n","\n","def features_selection(train, test, x_column, y_column, random_state=None):\n","    from itertools import combinations\n","    import multiprocessing\n","    import functools\n","\n","    predicts, total = [], len(x_column)\n","    # np.random.seed(random_state)\n","    # np.random.shuffle(x_column)\n","\n","    for i in range(total, 0, -1):\n","        print(f'>>>\\r{round((1-((i-1)/total)) * 100, 1)}% complete', end='')\n","        cb = sum([list(map(list, combinations(x_column, y))) for y in range(i, i + 1)], [])\n","\n","        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n","            pd = pool.map(functools.partial(_selection, train, test, x_column, y_column, random_state), cb)\n","            pd = np.array(pd, dtype=object)\n","            pool.close()\n","            pool.join()\n","\n","        if i == total:\n","            x_column = x_column[np.argsort(pd[:,2][0])[::-1]]\n","\n","        index_max = np.argmax(pd[:,0])\n","        index_del = [i for i, item in enumerate(x_column) if item not in pd[index_max][1]]\n","\n","        predicts.append(pd[index_max])\n","        x_column = np.delete(x_column, index_del)\n","\n","    predicts = np.array(predicts, dtype=object)\n","    predicts = predicts[predicts[:,0].argsort()][::-1]\n","    index_max = np.argmax(predicts[:,0])\n","    return (predicts, index_max)\n","\n","def _selection(*args):\n","    train, test, x_column, y_column, random_state, features = args\n","\n","    x_train, y_train = train[features].values, train[y_column].values\n","    x_test, y_test = test[features].values, test[y_column].values\n","\n","    clf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=random_state)\n","    clf.fit(x_train, np.squeeze(y_train))\n","\n","    cr = classification_report(y_test, clf.predict(x_test), output_dict=True, zero_division=True)\n","    return [cr['macro avg']['f1-score'], features, clf.feature_importances_]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3vfywc6GkmL","colab_type":"code","colab":{}},"source":["### READ ###\n","df = pd.read_csv('InvoicedDocuments_v5.csv', sep=';', na_values=['N/I'], parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'])\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)\n","\n","\n","### FILTERS ###\n","df.dropna(subset=['ClearingDate', 'PaymentTerms'], inplace=True)\n","\n","df = df[(df['DueDate'] > df['DocumentDate']) & (df['ClearingDate'] > df['DocumentDate'])]\n","df = df[df['DocumentAmount'] > 1000]\n","df.reset_index(drop=True, inplace=True)\n","\n","\n","### PREPROCESSING ###\n","df.fillna(-1, inplace=True)\n","df['CustomerRegion'] = df['CustomerRegion'].apply(lambda x: -1 if str(x).replace('-', '').isnumeric() else x)\n","\n","integer_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","df[integer_cols] = df[integer_cols].apply(pd.to_numeric, downcast='integer')\n","\n","category_cols = ['CorporateDivision', 'CustomerRegion', 'PaymentTerms']\n","df[category_cols] = df[category_cols].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","# df[category_cols] = df[category_cols].apply(lambda x: [int(y) if str(y).replace('-', '').isnumeric() else int(''.join(format(ord(w), '') for w in str(y))) for y in x])\n","\n","\n","### FEATURE GENERATION ###\n","amount_cols = ['InvoicedAmount', 'PaidAmount', 'PaidPastAmount', 'OpenAmount', 'PastDueAmount']\n","count_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","\n","for amount, count in zip(amount_cols, count_cols):\n","    ratio_col = 'Ratio' + amount + count\n","    df[ratio_col] = df[amount] / df[count]\n","    df[ratio_col].fillna(0, inplace=True)\n","\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'DayOfYear'] = pd.DatetimeIndex(df[col]).dayofyear\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'MonthEnd'] = df[col] + pd.offsets.MonthEnd(1)\n","\n","df['DueDateToClearingDate'] = (df['ClearingDate'] - df['DueDate']).astype('timedelta64[D]').astype(int)\n","df['DueDateToMonthEnd'] = (df['DueDateMonthEnd'] - df['DueDate']).astype('timedelta64[D]').astype(int)\n","df['DocumentDateToDueDate'] = (df['DueDate'] - df['DocumentDate']).astype('timedelta64[D]').astype(int)\n","df['DocumentDateToMonthEnd'] = (df['DocumentDateMonthEnd'] - df['DocumentDate']).astype('timedelta64[D]').astype(int)\n","\n","df, labels = setup_buckets(df, bins=[1, 8, 15, 22, 29], columns=['DueDateToClearingDate'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4W17zwgBG7rB","colab_type":"code","colab":{}},"source":["df, labels = setup_buckets(df, bins=[1, 4, 7], columns=['DueDateToClearingDate'])\n","# df, labels = setup_buckets(df, bins=[1], columns=['DueDateToClearingDate'])\n","\n","# df.isnull().sum()\n","df['CustomerKey'].unique()\n","\n","# df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OT0CYcZZftD9","colab_type":"code","colab":{}},"source":["y_column = np.array(['DueDateToClearingDateBucketCT'])\n","x_column = np.array([\n","                     'CompanyKey',\n","                     'CustomerKey',\n","                     'CorporateDivision',\n","                     'CustomerRegion',\n","                     'PaymentTerms',\n","                     'DocumentDateDay',\n","                     'DocumentDateWeekDay',\n","                     'DocumentDateDayOfYear',\n","                     'DocumentDateToMonthEnd',\n","                     'DueDateDay',\n","                     'DueDateWeekDay',\n","                     'DueDateDayOfYear',\n","                     'DueDateToMonthEnd',\n","                     'DocumentDateToDueDate',\n","                     'DocumentAmount',\n","                     'AvgDSOPastDueDocuments',\n","                     'AvgPastDueDays',\n","                     'PaidDocuments',\n","                     'PaidAmount',\n","                     'InvoicedDocuments',\n","                     'InvoicedAmount',\n","                     'OpenDocuments',\n","                     'OpenAmount',\n","                     'PaidPastDocuments',\n","                     'PaidPastAmount',\n","                     'PastDueDocuments',\n","                     'PastDueAmount',\n","                     'RatioInvoicedAmountInvoicedDocuments',\n","                     'RatioPaidAmountPaidDocuments',\n","                     'RatioPaidPastAmountPaidPastDocuments',\n","                     'RatioPastDueAmountPastDueDocuments',\n","                     'RatioOpenAmountOpenDocuments',\n","                     ])\n","\n","\n","train, test = split_data_month_window(df, col='DocumentDate', date='2020-08-01', month_window=2)\n","\n","plot_data_distribution(train['DueDateToClearingDateBucket'], test['DueDateToClearingDateBucket'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eFm8S53mYZ0","colab_type":"code","colab":{}},"source":["x_train, y_train = train[x_column].values, train[y_column].values\n","x_test, y_test = test[x_column].values, test[y_column].values\n","\n","clf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=seed, n_jobs=-1)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","predict = clf.predict(x_test)\n","plot_confuncion_matrix(y_test, predict)\n","\n","print(classification_report(y_test, predict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBIj1vBJOfR_","colab_type":"code","colab":{}},"source":["plot_feature_importance(x_column, clf.feature_importances_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naGFGC6IJFuT","colab_type":"code","colab":{}},"source":["# >>> Max f1-score: 0.6142799470165612, ['CompanyKey', 'DueDateWeekDay', 'DocumentDateToDueDate', 'PaymentTerms', 'CorporateDivision', 'AvgDSOPastDueDocuments', 'DocumentDateWeekDay', 'RatioInvoicedAmountInvoicedDocuments', 'RatioPastDueAmountPastDueDocuments', 'PaidDocuments', 'DueDateDay', 'CustomerKey', 'DocumentDateDay', 'CustomerRegion']\n","\n","predicts, index_max = features_selection(train, test, x_column, y_column, random_state=seed)\n","\n","print(f'\\n>>> Max f1-score: {predicts[index_max,0]}, {predicts[index_max,1]}')\n","print(f'\\n>>> Attempts:\\n{predicts}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJDgYzEvzbNf","colab_type":"code","colab":{}},"source":["# from sklearn.model_selection import train_test_split\n","# import tensorflow as tf\n","\n","# # !nvidia-smi\n","# # !pip install -q tensorflow-gpu\n","\n","# def binary_encoding(df, cols):\n","#     for col in cols:\n","#         bincol = np.array([str('{0:b}'.format(x)) for x in df[col[1]].values])\n","#         header = np.array([f'{col[1]}{i}' for i in range(col[0])])\n","#         newcol = np.zeros((bincol.shape[0], col[0]), dtype=np.int8)\n","\n","#         for i in range(bincol.shape[0]):\n","#             a = np.array(list(bincol[i]), dtype=np.int8)\n","#             newcol[i][col[0] - len(a):] = a\n","\n","#         df2 = pd.DataFrame(newcol, columns=header)\n","#         df.reset_index(drop=True, inplace=True)\n","#         df = pd.concat([df, df2], axis=1)\n","#         df.drop(columns=[col[1]], inplace=True)\n","#     return df\n","\n","\n","# train_nn = train_res[x_column].copy()\n","# test_nn = test[x_column].copy()\n","\n","# cols = [(32, 'CompanyKey'),\n","#         (12, 'CorporateDivision'),\n","#         (32, 'CustomerKey'),\n","#         (12, 'CustomerRegion'),\n","#         (12, 'PaymentTerms'),\n","#         (9, 'DocumentDateDayOfYear'),\n","#         (3, 'DocumentDateWeekDay'),\n","#         (5, 'DocumentDateDay'),\n","#         (9, 'DueDateDayOfYear'),\n","#         (3, 'DueDateWeekDay'),\n","#         (5, 'DueDateDay'),\n","#         (5, 'DueDateToMonthEnd'),\n","#         (9, 'DocumentDateToDueDate'),\n","#         (5, 'DocumentDateToMonthEnd'),]\n","\n","# train_nn = binary_encoding(train_nn, cols)\n","# test_nn = binary_encoding(test_nn, cols)\n","\n","\n","# x_train, x_valid, y_train, y_valid = train_test_split(train_nn.values, train_res[y_column].values,\n","#                                                       test_size=0.1, shuffle=True, random_state=seed,\n","#                                                       stratify=train_res[y_column].values)\n","\n","# y_train_categorical = tf.keras.utils.to_categorical(y_train)\n","# y_valid_categorical = tf.keras.utils.to_categorical(y_valid)\n","\n","\n","\n","# def create_model():\n","#     model = tf.keras.models.Sequential(name='cubricks')\n","\n","#     model.add(tf.keras.layers.Input(shape=train_nn.values.shape[1]))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(2, activation='softmax'))\n","#     return model\n","\n","\n","# model = create_model()\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-8, amsgrad=True),\n","#               loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1, reduction=tf.keras.losses.Reduction.SUM),\n","#               metrics=['accuracy'])\n","\n","# model.summary()\n","# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=20, restore_best_weights=True, verbose=1)]\n","\n","# model.fit(x_train,\n","#           y_train_categorical,\n","#           validation_data=(x_valid, y_valid_categorical),\n","#           callbacks=callbacks,\n","#           batch_size=256,\n","#           epochs=10000,\n","#           verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEXOBKmB1DUX","colab_type":"code","colab":{}},"source":["# predict = np.argmax(model.predict(test_nn.values), axis=1)\n","# plot_confuncion_matrix(y_test, predict)\n","\n","# # plot_feature_importance(x_column, clf.feature_importances_)\n","# print(classification_report(y_test, predict))"],"execution_count":null,"outputs":[]}]}