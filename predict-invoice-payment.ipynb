{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMLMqfCzYzoHoa2v8IHVdXr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oe4itJD5QlwP","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_L2WUirTE9Kp","colab_type":"code","colab":{}},"source":["!pip -q install imbalanced-learn --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jt-ZWeIcE-cu","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYxkDaUCF_mu","colab_type":"code","colab":{}},"source":["def plot_data_distribution(train, test, labels=None):\n","    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 2))\n","    sns.countplot(np.squeeze(train), ax=ax[0]).set_title(f'Train - {col}')\n","    sns.countplot(np.squeeze(test), ax=ax[1]).set_title(f'Test - {col}')\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confuncion Matrix'):\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax).set_title(title)\n","\n","def plot_feature_importance(features, importances, plot=True):\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(x_train.shape[1]):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    if plot:\n","        plt.figure(figsize=(10, 8))\n","        plt.barh(range(x_train.shape[1]), importances[indices])\n","        plt.yticks(range(x_train.shape[1]), features[indices])\n","        plt.title('Feature Importance')\n","        plt.gca().invert_yaxis()\n","        plt.show()\n","\n","def setup_buckets(df, bins, columns):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]}-{bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    labels[0], labels[-1] = 'on-time', labels[-1].replace('-inf', '+')\n","\n","    for col in columns:\n","        df[col + 'Bucket'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","        df[[col + 'BucketCT']] = df[[col + 'Bucket']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return (df, labels)\n","\n","def split_data_month_window(df, col, date, month_window):\n","    date_0 = pd.to_datetime(date)\n","    date_1 = date_0 - pd.DateOffset(months=month_window)\n","    date_2 = date_0 + pd.DateOffset(months=1)\n","    train = df[(df[col] >= date_1) & (df[col] < date_0)]\n","    test = df[(df[col] >= date_0) & (df[col] < date_2)]\n","    return train, test\n","\n","def resample(df, x_column, y_column, func):\n","    dtypes = df[x_column].dtypes.to_dict()\n","    dtypes.update(df[y_column].dtypes.to_dict())\n","\n","    x, y = df[x_column].values, df[y_column].values\n","    x, y = func.fit_resample(x, y)\n","    xy = np.concatenate((x, np.expand_dims(y, axis=1)), axis=1)\n","    \n","    data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","    data = data.astype(dtypes)\n","    return data\n","\n","\n","def features_selection(train, test, x_column, y_column, random_state=None):\n","    from itertools import combinations\n","    import multiprocessing\n","    import functools\n","\n","    total = len(x_column)\n","    predicts = []\n","\n","    for i in range(total, 0, -1):\n","        print(f'>>>\\r{round((1-((i-1)/total)) * 100, 1)}% complete', end='')\n","        cb = sum([list(map(list, combinations(x_column, y))) for y in range(i, i + 1)], [])\n","\n","        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n","            pd = pool.map(functools.partial(_selection, train, test, x_column, y_column, random_state), cb)\n","            pd = np.array(pd, dtype=object)\n","            pool.close()\n","            pool.join()\n","\n","        if i == total:\n","            x_column = x_column[np.argsort(pd[:,2][0])[::-1]]\n","\n","        index_max = np.argmax(pd[:,0])\n","        index_del = [i for i, item in enumerate(x_column) if item not in pd[index_max][1]]\n","\n","        predicts.append(pd[index_max])\n","        x_column = np.delete(x_column, index_del)\n","\n","    predicts = np.array(predicts, dtype=object)\n","    predicts = predicts[predicts[:,0].argsort()][::-1]\n","    index_max = np.argmax(predicts[:,0])\n","    return (predicts, index_max)\n","\n","def _selection(*args):\n","    train, test, x_column, y_column, random_state, features = args\n","\n","    x_train, y_train = train[features].values, train[y_column].values\n","    x_test, y_test = test[features].values, test[y_column].values\n","\n","    clf = RandomForestClassifier(n_estimators=1, criterion='entropy', random_state=random_state)\n","    clf.fit(x_train, np.squeeze(y_train))\n","\n","    cr = classification_report(y_test, clf.predict(x_test), output_dict=True, zero_division=True)\n","    return [cr['macro avg']['f1-score'], features, clf.feature_importances_]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3vfywc6GkmL","colab_type":"code","colab":{}},"source":["### READ ###\n","df = pd.read_csv('InvoicedDocuments_v4.csv', sep=';', na_values=['N/I'], parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'])\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)\n","\n","\n","### FILTERS ###\n","df.drop(columns=['CustomerLastCreditReview'], inplace=True)\n","df.dropna(subset=['ClearingDate', 'PaymentTerms'], inplace=True)\n","\n","df = df[df['DocumentAmount'] > 1000]\n","df = df[(df['DueDate'] > df['DocumentDate']) & (df['ClearingDate'] > df['DocumentDate'])]\n","df.reset_index(drop=True, inplace=True)\n","\n","\n","### PREPROCESSING ###\n","df.fillna(-1, inplace=True)\n","df['CustomerRegion'] = df['CustomerRegion'].apply(lambda x: -1 if str(x).replace('-', '').isnumeric() else x)\n","\n","integer_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","df[integer_cols] = df[integer_cols].apply(pd.to_numeric, downcast='integer')\n","\n","\n","### FEATURE GENERATION ###\n","category_cols = ['CustomerRegion', 'PaymentTerms']\n","df[category_cols] = df[category_cols].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","# df[category_cols] = df[category_cols].apply(lambda x: [int(y) if str(y).replace('-', '').isnumeric() else int(''.join(format(ord(w), '') for w in str(y))) for y in x])\n","\n","\n","amount_cols = ['InvoicedAmount', 'PaidAmount', 'PaidPastAmount', 'OpenAmount', 'PastDueAmount']\n","count_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","\n","for amount, count in zip(amount_cols, count_cols):\n","    ratio_col = 'Ratio' + amount + count\n","    df[ratio_col] = df[amount] / df[count]\n","    df[ratio_col].fillna(0, inplace=True)\n","\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'DayOfYear'] = pd.DatetimeIndex(df[col]).dayofyear\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'MonthEnd'] = df[col] + pd.offsets.MonthEnd(1)\n","\n","df['DueDateToClearingDate'] = (df['ClearingDate'] - df['DueDate']).astype('timedelta64[D]').astype(int)\n","df['DueDateToMonthEnd'] = (df['DueDateMonthEnd'] - df['DueDate']).astype('timedelta64[D]').astype(int)\n","df['DocumentDateToDueDate'] = (df['DueDate'] - df['DocumentDate']).astype('timedelta64[D]').astype(int)\n","df['DocumentDateToMonthEnd'] = (df['DocumentDateMonthEnd'] - df['DocumentDate']).astype('timedelta64[D]').astype(int)\n","\n","df, labels = setup_buckets(df, bins=[1, 8, 15, 22, 29], columns=['DueDateToClearingDate'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4W17zwgBG7rB","colab_type":"code","colab":{}},"source":["df, labels = setup_buckets(df, bins=[1, 4, 7], columns=['DueDateToClearingDate'])\n","# df, labels = setup_buckets(df, bins=[1], columns=['DueDateToClearingDate'])\n","\n","# df.isnull().sum()\n","# df['CustomerKey'].unique()\n","\n","# df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OT0CYcZZftD9","colab_type":"code","colab":{}},"source":["y_column = np.array(['DueDateToClearingDateBucketCT'])\n","x_column = np.array([\n","                     'CompanyKey',\n","                     'CustomerKey',\n","                     'CustomerRegion',\n","                     'PaymentTerms',\n","                     'DocumentAmount',\n","                     'InvoicedDocuments',\n","                     'InvoicedAmount',\n","                     'PaidDocuments',\n","                     'PaidAmount',\n","                     'PaidPastDocuments',\n","                     'PaidPastAmount',\n","                     'OpenDocuments',\n","                     'OpenAmount',\n","                     'PastDueDocuments',\n","                     'PastDueAmount',\n","                     'AvgDSOPastDueDocuments',\n","                     'PastDueDays',\n","                     'RatioInvoicedAmountInvoicedDocuments',\n","                     'RatioPaidAmountPaidDocuments',\n","                     'RatioPaidPastAmountPaidPastDocuments',\n","                     'RatioOpenAmountOpenDocuments',\n","                     'RatioPastDueAmountPastDueDocuments',\n","                     'DocumentDateDayOfYear',\n","                     'DocumentDateWeekDay',\n","                     'DocumentDateDay',\n","                     'DueDateDayOfYear',\n","                     'DueDateWeekDay',\n","                     'DueDateDay',\n","                     'DueDateToMonthEnd',\n","                     'DocumentDateToDueDate',\n","                     'DocumentDateToMonthEnd',\n","                     ])\n","\n","# x_column = np.array([\n","#                     #  'CompanyKey',\n","#                     #  'CustomerKey',\n","#                     #  'CustomerRegion',\n","#                     #  'DueDateWeekDay',\n","#                     #  'DocumentDateToDueDate',\n","#                      'CompanyKey',\n","#                      'PaymentTerms',\n","#                      'DueDateWeekDay',\n","#                      'CustomerKey',\n","#                      'DocumentDateToDueDate',\n","#                      'CustomerRegion',\n","#                      ])\n","\n","\n","train, test = split_data_month_window(df, col='DocumentDate', date='2020-06-01', month_window=12)\n","\n","plot_data_distribution(train['DueDateToClearingDateBucket'], test['DueDateToClearingDateBucket'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eFm8S53mYZ0","colab_type":"code","colab":{}},"source":["x_train, y_train = train[x_column].values, train[y_column].values\n","x_test, y_test = test[x_column].values, test[y_column].values\n","\n","clf = RandomForestClassifier(n_estimators=1, criterion='entropy', random_state=seed, n_jobs=-1)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","predict = clf.predict(x_test)\n","plot_confuncion_matrix(y_test, predict)\n","\n","# plot_feature_importance(x_column, clf.feature_importances_)\n","print(classification_report(y_test, predict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naGFGC6IJFuT","colab_type":"code","colab":{}},"source":["predicts, index_max = features_selection(train, test, x_column, y_column, random_state=seed)\n","\n","print(f'\\n>>> Max f1-score: {predicts[index_max,0]}, {predicts[index_max,1]}')\n","print(f'\\n>>> Attempts:\\n{predicts}')"],"execution_count":null,"outputs":[]}]}