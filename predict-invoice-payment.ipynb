{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":["VWOcTzbE34mt","c96ZzkN833rD","5_VPcHkK4N8Y","feVuEH6S4nby","meArwJng4sqX","5QIpmU6D4xRc","qu0szwafpEFt","sme8NgED53cw"],"authorship_tag":"ABX9TyNMYgzgRg6LvwYPY+W1tQgn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"F-jyFOsoxIMQ"},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWOcTzbE34mt"},"source":["## Modules"]},{"cell_type":"code","metadata":{"id":"Px0shqnqxJFf"},"source":["!pip -q install imbalanced-learn xgboost --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDaPtZ4YxJfB"},"source":["from sklearn.preprocessing import RobustScaler\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","SEED = 42"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c96ZzkN833rD"},"source":["## Plot Functions"]},{"cell_type":"code","metadata":{"id":"j-Qnt9bzx2Q7"},"source":["def plot_countplot(df, cols=[0], title=None, rotation=0):\n","    for col in cols:\n","        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 2))\n","        g = sns.countplot(x=np.squeeze(df[col] if isinstance(col, str) else df[:,col]), ax=ax)\n","        g.set_xticklabels(labels=g.get_xticklabels(), rotation=rotation)\n","        g.set_title(title)\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confusion Matrix', report=True):\n","    if report: print(classification_report(y_test, y_predict, digits=4, zero_division=True))\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    g = sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax)\n","    g.set_title(title)\n","\n","def plot_feature_importance(features, importances):\n","    features = np.array(features)\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(len(features)):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    plt.figure(figsize=(10, 8))\n","    plt.barh(range(len(features)), importances[indices])\n","    plt.yticks(range(len(features)), features[indices])\n","    plt.title('Feature Importance')\n","    plt.gca().invert_yaxis()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_VPcHkK4N8Y"},"source":["## Data Manager"]},{"cell_type":"code","metadata":{"id":"az4Gw8Dxx4JM"},"source":["def setup_buckets(df, col, bins, sufix='Bkt'):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]} to {bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    df[f'{col}{sufix}Label'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","    df[[f'{col}{sufix}']] = df[[f'{col}{sufix}Label']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return df, labels\n","\n","def split_data(df, date_column, split_date, train_months=6, test_months=1):\n","    split_date = pd.to_datetime(split_date)\n","    train_date = split_date - pd.DateOffset(months=train_months)\n","    test_date = split_date + pd.DateOffset(months=test_months)\n","\n","    df_ranged = df[(df[date_column] >= train_date) & (df[date_column] < test_date)].copy()\n","    df_ranged.reset_index(drop=True, inplace=True)\n","\n","    train = df_ranged[df_ranged[date_column] < split_date]\n","    test = df_ranged[df_ranged[date_column] >= split_date]\n","    return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feVuEH6S4nby"},"source":["## Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"_GdAt2JJ4cFg"},"source":["def preprocess_data(train, test, x_column, y_column, resample=None, nominal_columns=None):\n","    train, test = train.copy(), test.copy()\n","\n","    def _resample(df, x_column, y_column, func):\n","        dtypes = df[x_column].dtypes.to_dict()\n","        dtypes.update(df[y_column].dtypes.to_dict())    \n","        x, y = df[x_column].values, df[y_column].values\n","\n","        try:\n","            x, y = func.fit_resample(x, y)\n","            y = np.expand_dims(y, axis=1)\n","        except:\n","            pass\n","\n","        xy = np.concatenate((x, y), axis=1)\n","        data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","        data = data.astype(dtypes)\n","        return data\n","\n","    if nominal_columns is not None:\n","        to_number = lambda x: [int(w) if str(w).isnumeric() else int(''.join(format(ord(c), '') for c in str(w).lower())) for w in x]\n","        train[nominal_columns] = train[nominal_columns].apply(lambda x: to_number(x))\n","        test[nominal_columns] = test[nominal_columns].apply(lambda x: to_number(x))\n","\n","    if resample is not None:\n","        train = _resample(train, x_column, y_column, resample)\n","\n","    x_train, y_train = train[x_column].values, train[y_column].values\n","    x_test, y_test = test[x_column].values, test[y_column].values\n","\n","    qt = RobustScaler(quantile_range=(25.0, 75.0))\n","    qt.fit(x_train, y_train)\n","\n","    x_train = qt.transform(x_train)\n","    x_test = qt.transform(x_test)\n","    return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"meArwJng4sqX"},"source":["## Feature Selection"]},{"cell_type":"code","metadata":{"id":"AsCc4LrMx5a2"},"source":["def feature_selection(train, test, x_column, y_column, nominal_columns=None, target_class='macro avg', random_state=None):\n","    insertion = lambda l, x: [l[:i] + [x] + l[i:] for i in range(len(l) + 1)]\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    x_column = x_column.tolist()\n","\n","    def _loop_selection(groups, score=-1, features=[]):\n","        scores_groups = []\n","\n","        for feature_group in groups:\n","            x_column = np.array(flatten(feature_group))\n","            x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column, nominal_columns)\n"," \n","            clf = RandomForestClassifier(256, criterion='entropy', class_weight='balanced', random_state=random_state, n_jobs=-1)\n","            clf.fit(x_train, np.squeeze(y_train))\n","            y_predict = clf.predict(x_test)\n","\n","            cr = classification_report(test[y_column].values, y_predict, output_dict=True, zero_division=True)\n","            scores_groups.append(cr[target_class]['f1-score'])\n","\n","        local_score = max(scores_groups)\n","        local_features = list(groups[scores_groups.index(local_score)])\n","        print(f'Score: {local_score:.8f} >>> {local_features}')\n","\n","        if local_score > score:\n","            return local_score, local_features\n","\n","        return score, features\n","\n","    in_score, in_features = _loop_selection([[flatten(x_column)]])\n","    score, features = _loop_selection(np.expand_dims(x_column, axis=1).tolist())\n","    x_column.remove(features[0])\n","\n","    for group in x_column:\n","        score, features = _loop_selection(insertion(features, group), score, features)\n","\n","    if in_score > score:\n","        return in_score, in_features\n","\n","    return score, features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QIpmU6D4xRc"},"source":["## Read Dataset"]},{"cell_type":"code","metadata":{"id":"NptR4CUnx6O6"},"source":["### Read dataset ###\n","df = pd.read_csv('InvoicedDocuments_full.csv', sep=';', parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'], low_memory=False)\n","df.sort_values(by=['DueDate'], ascending=False, ignore_index=False, inplace=True)\n","\n","### Full fill ClearingDate column ###\n","df['Open'] = df['ClearingDate'].isnull() * 1\n","\n","current_last_month = pd.to_datetime('now') - pd.DateOffset(months=1) + pd.tseries.offsets.MonthEnd(1)\n","df.loc[(df['Open'] == 1) & (df['DueDate'] < current_last_month), 'ClearingDate'] = current_last_month\n","\n","### 'Days to' columns ###\n","date_int = lambda x: x.astype('timedelta64[D]')#.astype(int)\n","df['DueDateToClearingDate'] = date_int(df['ClearingDate'] - df['DueDate'])\n","\n","### Date columns ###\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","\n","### Full fill customer columns ###\n","for col in ['CustomerShipToKey', 'CustomerSoldToKey']:\n","    df[col] = df[col].fillna(df['CustomerKey'])\n","\n","for col in ['CustomerRegion']:\n","    is_numeric = lambda x: np.nan if str(x).replace('-', '').isnumeric() else x\n","    df[col] = df[col].apply(is_numeric).fillna(df[col].value_counts().idxmax())\n","\n","### Ratio columns ###\n","later_columns = ['TotalLatePaidInvoices', 'TotalPendingLateInvoices', 'SumAmountLatePaidInvoices', 'SumAmountPendingLateInvoices']\n","total_columns = ['TotalPaidInvoices', 'TotalPendingInvoices', 'SumAmountPaidInvoices', 'SumAmountPendingInvoices']\n","ratio_columns = ['RatioLatePaidInvoices', 'RatioPendingLateInvoices', 'RatioAmountPaidLateInvoices', 'RatioAmountPendingLateInvoices']\n","\n","for l, t, r in zip(later_columns, total_columns, ratio_columns):\n","    df[r] = (df[l] / df[t]).fillna(0)\n","\n","average_columns = ['AvgDaysLatePaidInvoices', 'AvgDaysLatePendingInvoices', 'StdevDaysLatePaidInvoices', 'StdevDaysLatePendingInvoices', 'PaymentFrequency']\n","df[average_columns] = df[average_columns].fillna(0)\n","\n","history_columns = later_columns + total_columns + ratio_columns + average_columns\n","df.loc[df[total_columns[0]].isnull(), history_columns] = -1\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTxSr3o858IY"},"source":["df, bin_class_names = setup_buckets(df, col='DueDateToClearingDate', bins=[4], sufix='Bin')\n","df, bucket_class_names = setup_buckets(df, col='DueDateToClearingDate', bins=[1, 4, 8], sufix='Bkt')\n","\n","plot_countplot(df, cols=['DueDateToClearingDateBinLabel', 'DueDateToClearingDateBktLabel'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"akOqrBUP5csk"},"source":["## Train and Test Data"]},{"cell_type":"code","metadata":{"id":"IaYnlq5g5-TN"},"source":["columns = {\n","    'target': [\n","        'DueDateToClearingDateBin',\n","    ],\n","    'numeric': [\n","        'StdevDaysLatePendingInvoices',\n","        'AvgDaysLatePendingInvoices',\n","        'StdevDaysLatePaidInvoices',\n","        'AvgDaysLatePaidInvoices',\n","        'RatioPendingLateInvoices',\n","        'RatioAmountPendingLateInvoices',\n","        'RatioLatePaidInvoices',\n","        'RatioAmountPaidLateInvoices',\n","        'PaymentFrequency',\n","        'PaymentTerm',\n","        'InvoiceAmount',\n","    ],\n","    'nominal': [\n","        'CompanyKey',\n","        'CustomerKey',\n","        'CustomerSoldToKey',\n","        'CustomerShipToKey',\n","        'CustomerRegion',\n","        'CorporateDivision',\n","    ],\n","    'date': [\n","        'DocumentDateWeekDay',\n","        'DueDateWeekDay',\n","        'DocumentDateDay',\n","        'DueDateDay',\n","    ],\n","}\n","\n","y_column = np.array(columns['target'])\n","x_column = np.array(columns['numeric'] + columns['nominal'] + columns['date'])\n","\n","train, test = split_data(df, date_column='DueDate', split_date='2020-01-01', train_months=6, test_months=1)\n","\n","# from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n","x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column,\n","                                                #    resample=ADASYN(sampling_strategy='auto', random_state=SEED),\n","                                                #    resample=SMOTE(sampling_strategy='auto', random_state=SEED),\n","                                                #    resample=BorderlineSMOTE(sampling_strategy='auto', kind='borderline-1', random_state=SEED),\n","                                                #    resample=BorderlineSMOTE(sampling_strategy='auto', kind='borderline-2', random_state=SEED),\n","                                                   nominal_columns=columns['nominal'] + columns['date'])\n","\n","plot_countplot(y_train, cols=[0], title='Train')\n","plot_countplot(y_test, cols=[0], title='Test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qu0szwafpEFt"},"source":["## Feature Selection"]},{"cell_type":"code","metadata":{"id":"lZ9TSdARpDy7"},"source":["# x_column = np.array([...])\n","\n","# score, features = feature_selection(train, test, x_column, y_column, nominal_columns=columns['nominal'] + columns['date'], random_state=SEED)\n","# print(f'\\nFinal score: {score:.8f} >>> {features}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sfPJxqyRrwjP"},"source":["## Methods"]},{"cell_type":"code","metadata":{"id":"nXVAX1Xi7Ep_"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(n_estimators=512,\n","                             criterion='entropy',\n","                             min_samples_split=8,\n","                             class_weight='balanced',\n","                             random_state=SEED,\n","                             n_jobs=-1)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IR3vIzeAkQJI"},"source":["# ### save csv ###\n","# cols = test.columns[:25].tolist() + ['DueDateToClearingDateBin']\n","# table = test[cols].copy().rename(columns={cols[-1]: 'GroundTruth'})\n","\n","# table.loc[test['Open'] == 1, 'ClearingDate'] = np.nan\n","# table.sort_values(by=['DueDate'], ascending=True, ignore_index=False, inplace=True)\n","\n","# y_prob = clf.predict_proba(x_test)\n","# table[['OnTime', 'Later4+']] = pd.DataFrame(y_prob, index=table.index)\n","\n","# table.to_csv(f\"{table['DueDate'].min().strftime('%Y-%m')}.csv\", sep=';', index=False)\n","\n","# ### plot feature importance ###\n","# plot_feature_importance(x_column, clf.feature_importances_)\n","\n","# ### apply threshold inference ###\n","# y_predict = (clf.predict_proba(x_test)[:,1] >= 0.9999).astype('int')\n","# plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kn508dWneip0"},"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', min_samples_split=8, class_weight='balanced'),\n","                        n_estimators=512,\n","                        random_state=SEED,\n","                        n_jobs=-1)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJUoraVspHQ7"},"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', min_samples_split=8, class_weight='balanced'),\n","                         n_estimators=512,\n","                         learning_rate=0.3,\n","                         random_state=SEED)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UNB9lOgt3iG"},"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","clf = GradientBoostingClassifier(n_estimators=512,\n","                                 learning_rate=0.3,\n","                                 min_samples_split=8,\n","                                 max_depth=6,\n","                                 random_state=SEED)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2ZsOuxAzbTs"},"source":["from xgboost import XGBClassifier\n","\n","clf = XGBClassifier(objective='binary:logistic',\n","                    n_estimators=512,\n","                    learning_rate=0.3,\n","                    scale_pos_weight=0.9,\n","                    seed=SEED,\n","                    n_jobs=-1)\n","\n","clf.fit(x_train, np.squeeze(y_train), eval_metric='logloss')\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Wk6fFsuaquc"},"source":["from imblearn.ensemble import BalancedRandomForestClassifier\n","\n","clf = BalancedRandomForestClassifier(n_estimators=512,\n","                                     criterion='entropy',\n","                                     min_samples_split=8,\n","                                     class_weight='balanced',\n","                                     sampling_strategy=0.7,\n","                                     random_state=SEED,\n","                                     n_jobs=-1)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3z4j8fma_Vg"},"source":["from imblearn.ensemble import BalancedBaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","clf = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', min_samples_split=8, class_weight='balanced'),\n","                                n_estimators=512,\n","                                sampling_strategy=0.7,\n","                                random_state=SEED,\n","                                n_jobs=-1)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEMl7vjCbjRu"},"source":["from imblearn.ensemble import RUSBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","clf = RUSBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', min_samples_split=8, class_weight='balanced'),\n","                         n_estimators=512,\n","                         sampling_strategy=0.7,\n","                         learning_rate=0.3,\n","                         random_state=SEED)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","y_predict = clf.predict(x_test)\n","\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_KFm2FluxY9"},"source":["from sklearn.neural_network import MLPClassifier\n","\n","clf = MLPClassifier(hidden_layer_sizes=(128, 64, 32),\n","                    activation='tanh', \n","                    batch_size='auto',\n","                    max_iter=1000,\n","                    solver='adam',\n","                    learning_rate_init=0.003,\n","                    learning_rate='adaptive',\n","                    alpha=0.0001,\n","                    early_stopping=True,\n","                    n_iter_no_change=20,\n","                    validation_fraction=0.1,\n","                    shuffle=True,\n","                    random_state=SEED,\n","                    verbose=False)\n","\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","y_predict = clf.predict(x_test)\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sme8NgED53cw"},"source":["## Neural Network"]},{"cell_type":"code","metadata":{"id":"TOBoF2QT54ui"},"source":["# from sklearn.utils.class_weight import compute_class_weight\n","# from sklearn.model_selection import train_test_split\n","\n","# import tensorflow_addons as tfa\n","# import tensorflow as tf\n","\n","# def make_model():\n","#     model = tf.keras.models.Sequential(name='cubricks')\n","#     model.add(tf.keras.layers.Input(shape=x_train.shape[1]))\n","\n","#     model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(512, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     # model.add(tf.keras.layers.Dense(32, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     # model.add(tf.keras.layers.Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","#     # # model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     # model.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(np.unique(y_train).shape[0], activation='softmax'))\n","\n","#     model.compile(\n","#         optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8, amsgrad=True),\n","#         loss=tf.keras.losses.CategoricalCrossentropy(),\n","#         # loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n","#         metrics=[tfa.metrics.F1Score(num_classes=np.unique(y_train).shape[0], average='weighted')])\n","\n","#     return model\n","\n","\n","# model = make_model()\n","# model.summary()\n","\n","# train, test = split_data_month_window(df, col='DueDate', date='2020-08-01', month_window=12)\n","# x_train, y_train, x_test, y_test = prepare_data(train, test, x_column, y_column, random_state=SEED)\n","\n","# x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=SEED, stratify=y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yb2UyYn957mE"},"source":["# BATCH = 256\n","# EPOCHS = 10000\n","# PATIENCE = 10000\n","\n","# tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', profile_batch=0)\n","\n","# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./logs/weights.hdf5',\n","#                                                 monitor='val_f1_score', mode='max',\n","#                                                 save_best_only=True, save_weights_only=True, verbose=1)\n","\n","# early_stopping = tf.keras.callbacks.EarlyStopping(patience=PATIENCE,\n","#                                                   monitor='val_f1_score', mode='max',\n","#                                                   restore_best_weights=True, verbose=1)\n","\n","# class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train[:,0])\n","# class_weight = dict(zip(np.unique(y_train), class_weight))\n","\n","# model = make_model()\n","# # model.load_weights(filepath='./logs/weights.hdf5')\n","\n","# model_history = model.fit(x_train, tf.keras.utils.to_categorical(y_train),\n","#                           validation_data=(x_valid, tf.keras.utils.to_categorical(y_valid)),\n","#                           epochs=EPOCHS, batch_size=BATCH, shuffle=True,\n","#                           callbacks=[checkpoint, early_stopping, tensorboard],\n","#                           class_weight=class_weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syDi2ilF58B_"},"source":["# def plot_model_history(history):\n","#     colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","#     plt.figure(figsize=(15, 3))\n","\n","#     for n, metric in enumerate(['loss', 'f1_score']):\n","#         plt.subplot(1, 2, n+1)\n","#         plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n","#         plt.plot(history.epoch, history.history['val_' + metric], color=colors[0], linestyle='--', label='Val')\n","#         plt.ylim([plt.ylim()[0], 1] if n > 0 else [0, plt.ylim()[1]])\n","#         plt.ylabel(metric.replace('_', ' ').capitalize())\n","#         plt.xlabel('Epoch')\n","#         plt.legend()\n","\n","\n","# plot_model_history(model_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2likXITi58Qz"},"source":["# model.load_weights(filepath='./logs/weights.hdf5')\n","\n","# y_predict = classifier_predict(model, x_test, threshold=0.5, network=True)\n","# plot_confuncion_matrix(y_test, y_predict)"],"execution_count":null,"outputs":[]}]}