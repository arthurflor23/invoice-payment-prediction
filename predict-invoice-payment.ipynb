{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbbBG00v8WubhNDVsAZlrZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SAswQok8f5Tv","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YW56hQ9nJEB9","colab_type":"code","colab":{}},"source":["!pip -q install pandas-profiling --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvrwRwKDcBnI","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2Y6l5uIT1Y9","colab_type":"code","colab":{}},"source":["def plot_feature_importance(features, importances, plot=False):\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(x_train.shape[1]):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    if plot:\n","        plt.figure(figsize=(10, 8))\n","        plt.barh(range(x_train.shape[1]), importances[indices])\n","        plt.yticks(range(x_train.shape[1]), features[indices])\n","        plt.title('Feature Importance')\n","        plt.gca().invert_yaxis()\n","        plt.show()\n","\n","\n","def plot_report(y_test, predict, labels=None, dummy=None, plot=False):\n","    print(f'Total items: {len(y_test)}')\n","\n","    if dummy is not None:\n","        dummy_cm = confusion_matrix(y_test, dummy)\n","        print(f'Dummy accuracy: {accuracy_score(y_test, dummy) * 100:.2f}%')\n","\n","    cm = confusion_matrix(y_test, predict)\n","    np.seterr(divide='ignore', invalid='ignore')\n","\n","    print(f'Model accuracy: {accuracy_score(y_test, predict) * 100:.2f}%\\n')\n","\n","    if plot:\n","        if dummy is not None:\n","            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n","            ax[0].set_title('Dummy prediction')\n","            ax[1].set_title('Model prediction')\n","            sns.heatmap(dummy_cm/dummy_cm.sum(axis=0), square=True, annot=True, fmt='.2%', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax[0])\n","            sns.heatmap(cm/cm.sum(axis=0), square=True, annot=True, fmt='.2%', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax[1])\n","        else:\n","            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n","            ax.set_title('Model prediction')\n","            sns.heatmap(cm/cm.sum(axis=0), square=True, annot=True, fmt='.2%', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n","\n","\n","def accuracy_by_class(y_test, predict, plot=True):\n","    cm = confusion_matrix(y_test, predict)\n","    acc = [len(y_test), cm.diagonal().sum() / len(y_test)]\n","    acc_class = []\n","\n","    for i, x in enumerate(np.unique(y_test)):\n","        acc_class.append([(y_test == x).sum(), cm.diagonal()[i] / (y_test == x).sum()])\n","\n","    acc = np.array(acc, dtype=np.object)\n","    acc_class = np.array(acc_class, dtype=np.object)\n","\n","    if plot:\n","        print(f'\\t\\tAcc.\\t Match\\t/ Total')\n","        print(f'Classes: \\t{acc[1]*100:.2f}%\\t({int(acc[0]*acc[1])}\\t/ {acc[0]})')\n","        for i, x in enumerate(acc_class):\n","            print(f'Class {i}: \\t{x[1]*100:.2f}%\\t({int(x[0]*x[1])}\\t/ {x[0]})')\n","\n","    return (acc, acc_class)\n","\n","\n","def split_data_month_window(df, col, date, month_window):\n","    date_0 = pd.to_datetime(date)\n","    date_1 = date_0 - pd.DateOffset(months=month_window)\n","    date_2 = date_0 + pd.DateOffset(months=1)\n","    train = df[(df[col] >= date_1) & (df[col] < date_0)]\n","    test = df[(df[col] >= date_0) & (df[col] < date_2)]\n","    return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9uuMervfTF5","colab_type":"code","colab":{}},"source":["df = pd.read_csv('InvoicedDocuments_v4.csv', sep=';', na_values=['N/I'])\n","\n","df.dropna(subset=['ClearingDate'], inplace=True)\n","df.fillna(0, inplace=True)\n","\n","\n","number_cols = ['CustomerRegion', 'PaymentTerms']\n","df[number_cols] = df[number_cols].apply(lambda x: [y if str(y).isnumeric() else int(''.join(format(ord(w), '') for w in str(y))) for y in x])\n","\n","\n","int_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n","\n","\n","date_cols = ['CustomerLastCreditReview', 'DocumentDate', 'DueDate', 'ClearingDate']\n","df[date_cols] = df[date_cols].apply(pd.to_datetime, errors='coerce')\n","\n","for col in date_cols[1:]:\n","    df[col + 'Month'] = pd.DatetimeIndex(df[col]).month\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","    df[col + 'MonthEnd'] = df[col] + pd.offsets.MonthEnd(1)\n","\n","\n","dividend_cols = ['InvoicedAmount', 'PaidAmount', 'PaidPastAmount', 'OpenAmount', 'PastDueAmount']\n","divisor_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","\n","for dividend, divisor in zip(dividend_cols, divisor_cols):\n","    ratio_col = 'Ratio' + dividend + divisor\n","    df[ratio_col] = df[dividend] / df[divisor]\n","    df[ratio_col].fillna(0, inplace=True)\n","\n","\n","source_cols = ['DocumentDate', 'DocumentDate', 'DueDate', 'DocumentDate', 'CustomerLastCreditReview']\n","target_cols = ['DueDate', 'DueDateMonthEnd', 'DueDateMonthEnd', 'ClearingDate', 'DocumentDate']\n","\n","for src, tgt in zip(source_cols, target_cols):\n","    delta_col = 'DaysTo' + tgt\n","    df[delta_col] = df[tgt] - df[src]\n","    df[delta_col].fillna(pd.Timedelta(seconds=0), inplace=True)\n","    df[delta_col] = df[delta_col].astype('timedelta64[D]').astype(int)\n","    df[delta_col] = df[delta_col].clip(lower=0)\n","\n","\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkbeB9RpjACX","colab_type":"code","colab":{}},"source":["df = df[df['DocumentDate'] <= df['DueDate']]\n","df = df[df['DocumentDate'] <= df['ClearingDate']]\n","\n","df = df[(df['ClearingDate'].dt.month - df['DueDate'].dt.month) >= 0]\n","df = df[(df['ClearingDate'].dt.month - df['DueDate'].dt.month) <= 1]\n","\n","df['AfterDueDateMonthEnd'] = (df['ClearingDate'] > df['DueDateMonthEnd']) * 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MXfbxc2LLNG","colab_type":"code","colab":{}},"source":["y_column = np.array(['AfterDueDateMonthEnd'])\n","features = np.array([\n","                     'AvgDSOPastDueDocuments',\n","                     'CompanyKey',\n","                     'CustomerKey',\n","                     'CustomerRegion',\n","                     'DaysToDocumentDate',\n","                     'DaysToDueDate',\n","                     'DaysToDueDateMonthEnd',\n","                     'DocumentAmount',\n","                     'DocumentDateDay',\n","                     'DocumentDateMonth',\n","                     'DocumentDateWeekDay',\n","                     'DueDateDay',\n","                     'DueDateMonth',\n","                     'DueDateWeekDay',\n","                     'InvoicedAmount',\n","                     'InvoicedDocuments',\n","                     'OpenAmount',\n","                     'OpenDocuments',\n","                     'PaidAmount',\n","                     'PaidDocuments',\n","                     'PaidPastAmount',\n","                     'PaidPastDocuments',\n","                     'PastDueAmount',\n","                     'PastDueDays',\n","                     'PastDueDocuments',\n","                     'PaymentTerms',\n","                     'RatioInvoicedAmountInvoicedDocuments',\n","                     'RatioOpenAmountOpenDocuments',\n","                     'RatioPaidAmountPaidDocuments',\n","                     'RatioPaidPastAmountPaidPastDocuments',\n","                     'RatioPastDueAmountPastDueDocuments',\n","                     ])\n","\n","\n","train, test = split_data_month_window(df, col='DocumentDate', date='2020-04-01', month_window=2)\n","\n","# t1, _ = split_data_month_window(df, col='DocumentDate', date='2020-07-01', month_window=2)\n","# train = train[train['AfterDueDateMonthEnd'] == 1].append(t1[t1['AfterDueDateMonthEnd'] == 0][75000:], ignore_index=False)\n","\n","# t1, _ = split_data_month_window(df, col='DocumentDate', date='2020-02-01', month_window=3)\n","# train = train.append(t1[t1['AfterDueDateMonthEnd'] == 1], ignore_index=False)\n","\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 2))\n","sns.countplot(train[y_column[0]], ax=ax[0]).set_title(\"Train - AfterDueDateMonthEnd\")\n","sns.countplot(test[y_column[0]], ax=ax[1]).set_title(\"Test - AfterDueDateMonthEnd\")\n","\n","\n","# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 5))\n","# sns.heatmap(train[features].corr(), square=True, annot=False, fmt='.1g', vmin=-1, vmax=1, center=0, cmap='Pastel1', ax=ax[0])\n","# sns.heatmap(test[features].corr(), square=True, annot=False, fmt='.1g', vmin=-1, vmax=1, center=0, cmap='Pastel1', ax=ax[1])\n","\n","x_train, y_train = train[features].values, train[y_column].values\n","x_test, y_test = test[features].values, test[y_column].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4Kr5hq8x9PE","colab_type":"code","colab":{}},"source":["from sklearn.utils import resample\n","\n","# Separate majority and minority classes\n","df_majority = train[train['AfterDueDateMonthEnd'] == 1]\n","df_minority = train[train['AfterDueDateMonthEnd'] == 0]\n"," \n","# Upsample minority class\n","df_minority_upsampled = resample(df_minority,\n","                                 replace=True,                  # sample with replacement\n","                                 n_samples=len(df_majority),    # to match majority class\n","                                 random_state=32)               # reproducible results\n"," \n","# Combine majority class with upsampled minority class\n","df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n"," \n","# Display new class counts\n","print(df_upsampled['AfterDueDateMonthEnd'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlEqCa39yn_a","colab_type":"code","colab":{}},"source":["x_train, y_train = df_upsampled[features].values, df_upsampled[y_column].values\n","\n","clf = LogisticRegression().fit(x_train, np.squeeze(y_train))\n","predict = clf.predict(x_test)\n","\n","print(np.unique(predict))\n","print('\\nModel prediction:')\n","_ = accuracy_by_class(y_test, predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W75CQMsCZWLG","colab_type":"code","colab":{}},"source":["# def select_features(train, test, features, y_column):\n","#     cb, predicts = [], []\n","\n","#     for y in range(len(features)):\n","#         for x in range(y, len(features)):\n","#             cb.append(cb[-1] + [features[x]] if x > y else [features[y]])\n","\n","#     cb = np.array(sorted(cb, key=len))\n","#     print(f'Combinations: {len(cb)}')\n","\n","#     for features_comb in cb:\n","#         random_forest = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=32)\n","\n","#         x_train, y_train = train[features_comb].values, train[y_column].values\n","#         x_test, y_test = test[features_comb].values, test[y_column].values\n","\n","#         random_forest.fit(x_train, np.squeeze(y_train))\n","#         predict = random_forest.predict(x_test)\n","\n","#         acc, acc_class = accuracy_by_class(y_test, predict, plot=False)\n","#         predicts.append([acc_class[1][1], features_comb])\n","\n","#     return np.array(predicts, dtype=object)\n","\n","\n","# Max accuracy: 0.33461261693451666, features: ['CompanyKey', 'CustomerKey', 'CustomerRegion', 'DaysToDocumentDate', 'DaysToDueDate', 'DaysToDueDateMonthEnd', 'DocumentAmount', 'DocumentDateDay', 'DocumentDateMonth', 'DocumentDateWeekDay', 'DueDateDay', 'DueDateMonth', 'DueDateWeekDay', 'InvoicedAmount', 'InvoicedDocuments', 'OpenAmount', 'OpenDocuments', 'PaidAmount', 'PaidDocuments', 'PaidPastAmount', 'PaidPastDocuments', 'PastDueAmount', 'PastDueDays', 'PastDueDocuments', 'PaymentTerms', 'RatioInvoicedAmountInvoicedDocuments', 'RatioOpenAmountOpenDocuments', 'RatioPaidAmountPaidDocuments']\n","# predicts = select_features(train, test, features, y_column)\n","\n","# print(f'\\nMax accuracy: {np.max(predicts[:,0])}, features: {predicts[np.argmax(predicts[:,0])][1]}')\n","# print(f'\\nAttempts:\\n{predicts}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSPEmwpgy9xR","colab_type":"code","colab":{}},"source":["# random_forest = RandomForestClassifier(n_estimators=10, criterion='entropy', min_weight_fraction_leaf=1e-4, random_state=32)\n","random_forest = RandomForestClassifier(n_estimators=15, criterion='entropy', random_state=32)\n","random_forest.fit(x_train, np.squeeze(y_train))\n","\n","predict = random_forest.predict(x_test)\n","\n","print('Dummy prediction:')\n","_ = accuracy_by_class(y_test, np.zeros(y_test.shape))\n","\n","print('\\nModel prediction:')\n","_ = accuracy_by_class(y_test, predict)\n","\n","# plot_report(y_test, predict, labels=[0, 1], dummy=np.zeros(predict.shape), plot=True)\n","# plot_feature_importance(features, random_forest.feature_importances_, plot=True)"],"execution_count":null,"outputs":[]}]}