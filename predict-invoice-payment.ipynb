{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4hiz9tgqEFsSLxZUYYCGP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SAswQok8f5Tv","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YW56hQ9nJEB9","colab_type":"code","colab":{}},"source":["!pip -q install imbalanced-learn --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvrwRwKDcBnI","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from imblearn.over_sampling import ADASYN\n","from imblearn.under_sampling import InstanceHardnessThreshold, RandomUnderSampler\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2Y6l5uIT1Y9","colab_type":"code","colab":{}},"source":["def plot_data_distribution(train, test):\n","    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 2))\n","    sns.countplot(np.squeeze(train), ax=ax[0]).set_title(f'Train - {col}')\n","    sns.countplot(np.squeeze(test), ax=ax[1]).set_title(f'Test - {col}')\n","\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confuncion Matrix'):\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax).set_title(title)\n","\n","\n","# def plot_feature_importance(features, importances, plot=False):\n","#     indices = np.argsort(importances)[::-1]\n","#     print(f'Feature ranking:')\n","\n","#     for f in range(x_train.shape[1]):\n","#         print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","#     if plot:\n","#         plt.figure(figsize=(10, 8))\n","#         plt.barh(range(x_train.shape[1]), importances[indices])\n","#         plt.yticks(range(x_train.shape[1]), features[indices])\n","#         plt.title('Feature Importance')\n","#         plt.gca().invert_yaxis()\n","#         plt.show()\n","\n","\n","def split_data_month_window(df, col, date, month_window):\n","    date_0 = pd.to_datetime(date)\n","    date_1 = date_0 - pd.DateOffset(months=month_window)\n","    date_2 = date_0 + pd.DateOffset(months=1)\n","    train = df[(df[col] >= date_1) & (df[col] < date_0)]\n","    test = df[(df[col] >= date_0) & (df[col] < date_2)]\n","    return train, test\n","\n","\n","def resample(df, x_column, y_column, func):\n","    df = df.copy()\n","    x, y = df[x_column].values, df[y_column].values\n","    x, y = func.fit_resample(x, y)\n","    xy = np.concatenate((x, np.expand_dims(y, axis=1)), axis=1)\n","    data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","    return data.apply(pd.to_numeric)\n","\n","\n","def features_selection(train, test, x_column, y_column):\n","    cb, predicts = [], []\n","\n","    for y in range(len(x_column)):\n","        for x in range(y, len(x_column)):\n","            cb.append(cb[-1] + [x_column[x]] if x > y else [x_column[y]])\n","\n","    cb = np.array(sorted(cb, key=len))\n","    print(f'Combinations: {len(cb)}')\n","\n","    for features_comb in cb:\n","        clf = RandomForestClassifier(n_estimators=40, criterion='entropy', min_weight_fraction_leaf=1e-4, random_state=seed)\n","\n","        x_train, y_train = train[features_comb].values, train[y_column].values\n","        x_test, y_test = test[features_comb].values, test[y_column].values\n","\n","        clf.fit(x_train, np.squeeze(y_train))\n","        predict = clf.predict(x_test)\n","\n","        cr = classification_report(y_test, predict, output_dict=True, zero_division=True)\n","        predicts.append([cr['macro avg']['f1-score'], features_comb])\n","\n","    predicts = np.array(predicts, dtype=object)\n","    argmax_index = np.argmax(predicts[:,0])\n","    return (predicts, argmax_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9uuMervfTF5","colab_type":"code","colab":{}},"source":["df = pd.read_csv('InvoicedDocuments_v4.csv', sep=';', na_values=['N/I'])\n","\n","df.dropna(subset=['ClearingDate'], inplace=True)\n","df.fillna(0, inplace=True)\n","\n","\n","number_cols = ['CustomerRegion', 'PaymentTerms']\n","df[number_cols] = df[number_cols].apply(lambda x: [y if str(y).isnumeric() else int(''.join(format(ord(w), '') for w in str(y))) for y in x])\n","\n","\n","int_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n","\n","\n","date_cols = ['CustomerLastCreditReview', 'DocumentDate', 'DueDate', 'ClearingDate']\n","df[date_cols] = df[date_cols].apply(pd.to_datetime, errors='coerce')\n","\n","for col in date_cols[1:]:\n","    df[col + 'Month'] = pd.DatetimeIndex(df[col]).month\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","    df[col + 'MonthEnd'] = df[col] + pd.offsets.MonthEnd(1)\n","\n","\n","dividend_cols = ['InvoicedAmount', 'PaidAmount', 'PaidPastAmount', 'OpenAmount', 'PastDueAmount']\n","divisor_cols = ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']\n","\n","for dividend, divisor in zip(dividend_cols, divisor_cols):\n","    ratio_col = 'Ratio' + dividend + divisor\n","    df[ratio_col] = df[dividend] / df[divisor]\n","    df[ratio_col].fillna(0, inplace=True)\n","\n","\n","source_cols = ['DocumentDate', 'DocumentDate', 'DueDate', 'DocumentDate', 'CustomerLastCreditReview']\n","target_cols = ['DueDate', 'DueDateMonthEnd', 'DueDateMonthEnd', 'ClearingDate', 'DocumentDate']\n","\n","for src, tgt in zip(source_cols, target_cols):\n","    delta_col = 'DaysTo' + tgt\n","    df[delta_col] = df[tgt] - df[src]\n","    df[delta_col].fillna(pd.Timedelta(seconds=0), inplace=True)\n","    df[delta_col] = df[delta_col].astype('timedelta64[D]').astype(int)\n","    df[delta_col] = df[delta_col].clip(lower=0)\n","\n","\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkbeB9RpjACX","colab_type":"code","colab":{}},"source":["df = df[df['DocumentDate'] <= df['DueDate']]\n","df = df[df['DocumentDate'] <= df['ClearingDate']]\n","\n","df = df[(df['ClearingDate'].dt.month - df['DueDate'].dt.month) >= 0]\n","df = df[(df['ClearingDate'].dt.month - df['DueDate'].dt.month) <= 1]\n","\n","df['AfterDueDateMonthEnd'] = (df['ClearingDate'] > df['DueDateMonthEnd']) * 1\n","df.head(100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MXfbxc2LLNG","colab_type":"code","colab":{}},"source":["y_column = np.array(['AfterDueDateMonthEnd'])\n","x_column = np.array([\n","                     'CompanyKey',\n","                     'CustomerKey',\n","                     'CustomerRegion',\n","                     'PaymentTerms',\n","                     'DocumentDateDay',\n","                     'DocumentDateMonth',\n","                     'DocumentDateWeekDay',\n","                     'DueDateDay',\n","                     'DueDateMonth',\n","                     'DueDateWeekDay',\n","                     'DaysToDocumentDate',\n","                     'DaysToDueDate',\n","                     'DaysToDueDateMonthEnd',\n","                     'AvgDSOPastDueDocuments',\n","                     'DocumentAmount',\n","                     'InvoicedAmount',\n","                     'InvoicedDocuments',\n","                     'OpenAmount',\n","                     'OpenDocuments',\n","                     'PaidAmount',\n","                     'PaidDocuments',\n","                     'PaidPastAmount',\n","                     'PaidPastDocuments',\n","                     'PastDueAmount',\n","                     'PastDueDays',\n","                     'PastDueDocuments',\n","                     'RatioInvoicedAmountInvoicedDocuments',\n","                     'RatioOpenAmountOpenDocuments',\n","                     'RatioPaidAmountPaidDocuments',\n","                     'RatioPaidPastAmountPaidPastDocuments',\n","                     'RatioPastDueAmountPastDueDocuments',\n","                     ])\n","\n","\n","train, test = split_data_month_window(df, col='DocumentDate', date='2020-06-01', month_window=2)\n","\n","plot_data_distribution(train[y_column], test[y_column])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoPCfHf3MexY","colab_type":"code","colab":{}},"source":["x_test, y_test = np.ones(test[y_column].values.shape), test[y_column].values\n","\n","print('>>> Dummy prediction <<<')\n","print(classification_report(y_test, x_test, zero_division=True))\n","\n","plot_confuncion_matrix(y_test, x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oHqsgsB1IqT","colab_type":"code","colab":{}},"source":["x_train, y_train = train[x_column].values, train[y_column].values\n","x_test, y_test = test[x_column].values, test[y_column].values\n","\n","clf = RandomForestClassifier(n_estimators=40, criterion='entropy', min_weight_fraction_leaf=1e-4, random_state=seed)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","predict = clf.predict(x_test)\n","\n","print('>>> Imbalanced model prediction <<<')\n","print(classification_report(y_test, predict))\n","\n","plot_confuncion_matrix(y_test, predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGmwYNjg4kJW","colab_type":"code","colab":{}},"source":["# train_res = resample(train, x_column, y_column, InstanceHardnessThreshold(random_state=seed, n_jobs=-1))\n","# train_res = resample(train, x_column, y_column, ADASYN(sampling_strategy=0.1, n_neighbors=250, random_state=seed))\n","# train_res = resample(train, x_column, y_column, RandomUnderSampler(replacement=True, random_state=seed))\n","\n","plot_data_distribution(train_res[y_column], test[y_column])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nh6TR138pSFJ","colab_type":"code","colab":{}},"source":["# x_column = np.array(['DaysToDueDateMonthEnd', 'AvgDSOPastDueDocuments', 'DocumentAmount', 'InvoicedAmount', 'InvoicedDocuments', 'OpenAmount', 'OpenDocuments', 'PaidAmount', 'PaidDocuments', 'PaidPastAmount', 'PaidPastDocuments', 'PastDueAmount'])\n","\n","x_train, y_train = train_res[x_column].values, train_res[y_column].values\n","x_test, y_test = test[x_column].values, test[y_column].values\n","\n","clf = RandomForestClassifier(n_estimators=40, criterion='entropy', min_weight_fraction_leaf=1e-4, random_state=seed)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","predict = clf.predict(x_test)\n","\n","print('>>> Balanced model prediction <<<')\n","print(classification_report(y_test, predict))\n","\n","plot_confuncion_matrix(y_test, predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W75CQMsCZWLG","colab_type":"code","colab":{}},"source":["predicts, argmax_index = features_selection(train_res, test, x_column, y_column)\n","\n","print(f'>>> Max f1-score: {predicts[argmax_index]}\\n>>> Attempts:\\n{predicts}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aonEA3qrGEQm","colab_type":"code","colab":{}},"source":["# from sklearn.model_selection import train_test_split\n","# import tensorflow as tf\n","\n","# # !nvidia-smi\n","# # !pip install -q tensorflow-gpu\n","\n","\n","# def binary_encoding(df, cols):\n","#     for col in cols:\n","#         bincol = np.array([str('{0:b}'.format(x)) for x in df[col[1]].values])\n","#         header = np.array([f'{col[1]}{i}' for i in range(col[0])])\n","#         newcol = np.zeros((bincol.shape[0], col[0]), dtype=np.int8)\n","\n","#         for i in range(bincol.shape[0]):\n","#             a = np.array(list(bincol[i]), dtype=np.int8)\n","#             newcol[i][col[0] - len(a):] = a\n","\n","#         df2 = pd.DataFrame(newcol, columns=header)\n","#         df.reset_index(drop=True, inplace=True)\n","#         df = pd.concat([df, df2], axis=1)\n","#         df.drop(columns=[col[1]], inplace=True)\n","\n","#     return df\n","\n","\n","# train_nn = train_res[x_column].copy()\n","# test_nn = test[x_column].copy()\n","\n","# cols = [(32, 'CompanyKey'),\n","#         (32, 'CustomerKey'),\n","#         (32, 'CustomerRegion'),\n","#         (32, 'PaymentTerms'),\n","#         (3, 'DocumentDateWeekDay'),\n","#         (3, 'DueDateWeekDay')]\n","\n","# train_nn = binary_encoding(train_nn, cols)\n","# test_nn = binary_encoding(test_nn, cols)\n","\n","\n","# x_train, x_valid, y_train, y_valid = train_test_split(train_nn.values, train_res[y_column].values,\n","#                                                       test_size=0.1, shuffle=True, random_state=seed,\n","#                                                       stratify=train_res[y_column].values)\n","\n","# y_train_categorical = tf.keras.utils.to_categorical(y_train)\n","# y_valid_categorical = tf.keras.utils.to_categorical(y_valid)\n","\n","\n","\n","# def create_model():\n","#     model = tf.keras.models.Sequential(name='cubricks')\n","\n","#     model.add(tf.keras.layers.Input(shape=train_nn.values.shape[1]))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001)))\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=True))\n","#     model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     model.add(tf.keras.layers.Dense(2, activation='softmax'))\n","#     return model\n","\n","\n","# model = create_model()\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-8, amsgrad=True),\n","#               loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1, reduction=tf.keras.losses.Reduction.SUM),\n","#               metrics=['accuracy'])\n","\n","# model.summary()\n","# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=20, restore_best_weights=True, verbose=1)]\n","\n","# model.fit(x_train,\n","#           y_train_categorical,\n","#           validation_data=(x_valid, y_valid_categorical),\n","#           callbacks=callbacks,\n","#           batch_size=256,\n","#           epochs=10000,\n","#           verbose=1)\n","\n","# predict = np.argmax(model.predict(test_nn.values), axis=1)\n","\n","# print('\\nNeural network model prediction:')\n","# _ = accuracy_by_class(y_test, predict)"],"execution_count":null,"outputs":[]}]}