{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOm/8ApRkL1tXz9DK22ElTI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4bnYcMJAZUoe"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhtPjoHTq7BZ"},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ipzzmxz2JqLN"},"source":["!pip -q install pandas-profiling imbalanced-learn tensorflow-gpu --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uqEobEKvHp8"},"source":["from sklearn.preprocessing import RobustScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeBrT3TrvQ6h"},"source":["def plot_countplot(df, cols, title=None, rotation=0):\n","    for col in cols:\n","        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 2))\n","        g = sns.countplot(x=np.squeeze(df[col] if isinstance(col, str) else df[:,col]), ax=ax)\n","        g.set_xticklabels(labels=g.get_xticklabels(), rotation=rotation)\n","        g.set_title(title)\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confusion Matrix', report=True):\n","    if report: print(classification_report(y_test, y_predict))\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    g = sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax)\n","    g.set_title(title)\n","\n","def plot_feature_importance(features, importances):\n","    features = np.array(features)\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(len(features)):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    plt.figure(figsize=(10, 8))\n","    plt.barh(range(len(features)), importances[indices])\n","    plt.yticks(range(len(features)), features[indices])\n","    plt.title('Feature Importance')\n","    plt.gca().invert_yaxis()\n","    plt.show()\n","\n","def setup_buckets(df, col, bins, sufix='Bucket'):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]} to {bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    df[col + sufix + 'Category'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","    df[[col + sufix]] = df[[col + sufix + 'Category']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return df\n","\n","def split_data_month_window(df, col, date, month_window):\n","    date_0 = pd.to_datetime(date)\n","    date_1 = date_0 - pd.DateOffset(months=month_window)\n","    date_2 = date_0 + pd.DateOffset(months=1)\n","\n","    train = df[(df[col] >= date_1) & (df[col] < date_0)]\n","    test = df[(df[col] >= date_0) & (df[col] < date_2)]\n","\n","    train.reset_index(drop=True, inplace=True)\n","    test.reset_index(drop=True, inplace=True)\n","    return train, test\n","\n","def prepare_data(train, test, x_column, y_column, random_state=None):\n","    randomize = np.arange(train.shape[0])\n","    np.random.seed(random_state)\n","    np.random.shuffle(randomize)\n","\n","    x_train, y_train = train[x_column].values[randomize], train[y_column].values[randomize]\n","    x_test, y_test = test[x_column].values, test[y_column].values\n","\n","    x_train, x_test = np.vectorize(np.log)(x_train + 1), np.vectorize(np.log)(x_test + 1)\n","\n","    qt = RobustScaler()\n","    qt.fit(np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test)))\n","\n","    x_train, x_test = qt.transform(x_train), qt.transform(x_test)\n","\n","    return x_train, y_train, x_test, y_test\n","\n","def classifier_predict(clf, x_test, threshold=0.5, network=False):\n","    y_predict = clf.predict(x_test) if network else clf.predict_proba(x_test)\n","    return (y_predict[:,1] >= threshold).astype('int')\n","\n","# def resample(df, x_column, y_column, func):\n","#     dtypes = df[x_column].dtypes.to_dict()\n","#     dtypes.update(df[y_column].dtypes.to_dict())\n","\n","#     x, y = df[x_column].values, df[y_column].values\n","#     x, y = func.fit_resample(x, y)\n","#     xy = np.concatenate((x, np.expand_dims(y, axis=1)), axis=1)\n","    \n","#     data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","#     data = data.astype(dtypes)\n","#     return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"faoC-yeX30uK"},"source":["### Read dataset ###\n","df = pd.read_csv('InvoicedDocuments_v6.csv', sep=';', na_values=['N/I'], parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'])\n","\n","### First filters ###\n","df.dropna(subset=['ClearingDate', 'PaymentTerms'], inplace=True)\n","df = df[(df['DueDate'] > df['DocumentDate']) & (df['ClearingDate'] > df['DocumentDate'])]\n","df = df[df['DocumentDate'] >= '2018-03-01']\n","\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)\n","df.reset_index(drop=True, inplace=True)\n","\n","### Fix numerical columns ###\n","for amount, count in zip(['InvoicedAmount', 'PaidAmount', 'PaidPastAmount', 'OpenAmount', 'PastDueAmount'],\n","                         ['InvoicedDocuments', 'PaidDocuments', 'PaidPastDocuments', 'OpenDocuments', 'PastDueDocuments']):\n","    df[amount] = df[amount] / df[count]\n","    df[[amount, count]] = df[[amount, count]].fillna(0)\n","\n","avg_cols = ['AvgDSOPastDueDocuments', 'AvgPastDueDays']\n","df[avg_cols] = df[avg_cols].fillna(0)\n","\n","### Extract date information ###\n","date_int = lambda x: x.astype('timedelta64[D]').astype(int)\n","\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'DayOfYear'] = pd.DatetimeIndex(df[col]).dayofyear\n","    df[col + 'Month'] = pd.DatetimeIndex(df[col]).month\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","\n","### Days to DueDate ###\n","df['DocumentDateToDueDate'] = date_int(df['DueDate'] - df['DocumentDate'])\n","\n","### Days to ClearingDate ###\n","df['DueDateToClearingDate'] = date_int(df['ClearingDate'] - df['DueDate'])\n","df['DocumentDateToClearingDate'] = date_int(df['ClearingDate'] - df['DocumentDate'])\n","\n","### Categorical columns ###\n","category_cols = ['CompanyKey', 'CustomerKey', 'CorporateDivision', 'CustomerRegion', 'PaymentTerms']\n","df['CustomerRegion'].fillna(df['CustomerRegion'].value_counts().idxmax(), inplace=True)\n","df[category_cols] = df[category_cols].apply(lambda x: pd.Categorical(x, ordered=False).codes)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ETafw3HDVMk"},"source":["df = setup_buckets(df, col='DueDateToClearingDate', bins=[1], sufix='Bin')\n","df = setup_buckets(df, col='DueDateToClearingDate', bins=[1, 3, 8], sufix='Bucket')\n","\n","plot_countplot(df, cols=['DueDateToClearingDateBinCategory', 'DueDateToClearingDateBucketCategory'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XSHX4q-JCTg"},"source":["y_column = np.array(['DueDateToClearingDateBin'])\n","x_column = np.array([\n","                    'AvgPastDueDays',\n","                    'AvgDSOPastDueDocuments',\n","                    # 'PaidDocuments',\n","                    # 'PaidAmount',\n","                    # 'InvoicedDocuments',\n","                    # 'InvoicedAmount',\n","                    # 'OpenDocuments',\n","                    # 'OpenAmount',\n","                    # 'PaidPastDocuments',\n","                    'PastDueAmount',\n","                    'PaidPastAmount',\n","                    # 'PastDueDocuments',\n","                    'DocumentAmount',\n","                    'DocumentDateToDueDate',\n","                        'CompanyKey',\n","                        'PaymentTerms',\n","                        'CorporateDivision',\n","                        'CustomerKey',\n","                        'CustomerRegion',\n","                    'DocumentDateDay',\n","                    'DocumentDateWeekDay',\n","                    'DocumentDateDayOfYear',\n","                    'DueDateDay',\n","                    'DueDateWeekDay',\n","                    'DueDateDayOfYear',\n","                    ])\n","\n","\n","train, test = split_data_month_window(df, col='DueDate', date='2020-08-01', month_window=12)\n","\n","\n","# from imblearn.over_sampling import SMOTE, ADASYN\n","# train = resample(train, x_column, y_column, ADASYN(random_state=seed))\n","\n","\n","x_train, y_train, x_test, y_test = prepare_data(train, test, x_column, y_column, random_state=seed)\n","\n","plot_countplot(train, cols=['DueDateToClearingDateBin'], title='Train')\n","plot_countplot(test, cols=['DueDateToClearingDateBin'], title='Test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6iwNiRfJsIu"},"source":["clf = RandomForestClassifier(n_estimators=50, criterion='entropy', class_weight='balanced', random_state=seed, n_jobs=-1)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","y_predict = classifier_predict(clf, x_test, threshold=0.5)\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzXzNE8He53H"},"source":["y_predict = classifier_predict(clf, x_test, threshold=0.9999)\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBUeM4lC-hm5"},"source":["plot_feature_importance(x_column, clf.feature_importances_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPlo_Pi6csSR"},"source":["# from pandas_profiling import ProfileReport\n","\n","# cols = np.concatenate((x_column, y_column), axis=0)\n","# df_train = np.concatenate((x_train, y_train), axis=1)\n","# df_test = np.concatenate((x_test, y_test), axis=1)\n","# df_local = np.concatenate((df_train, df_test), axis=0)\n","\n","# df_local = pd.DataFrame(df_local, columns=cols)\n","# profile = ProfileReport(df_local)\n","\n","# profile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNlVeilQcFCu"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow_addons as tfa\n","import tensorflow as tf\n","\n","def make_model():\n","    model = tf.keras.models.Sequential(name='cubricks')\n","    model.add(tf.keras.layers.Input(shape=x_train.shape[1]))\n","\n","    model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","    model.add(tf.keras.layers.PReLU())\n","    model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","    # model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","    model.add(tf.keras.layers.Dense(512, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","    model.add(tf.keras.layers.PReLU())\n","    model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","    # model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","    model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","    model.add(tf.keras.layers.PReLU())\n","    model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","    model.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","    model.add(tf.keras.layers.PReLU())\n","    model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","    model.add(tf.keras.layers.Dense(np.unique(y_train).shape[0], activation='softmax'))\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8, amsgrad=True),\n","        loss=tf.keras.losses.CategoricalCrossentropy(),\n","        # loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n","        metrics=[tfa.metrics.F1Score(num_classes=np.unique(y_train).shape[0], average='weighted')])\n","\n","    return model\n","\n","\n","model = make_model()\n","model.summary()\n","\n","train, test = split_data_month_window(df, col='DueDate', date='2020-08-01', month_window=12)\n","x_train, y_train, x_test, y_test = prepare_data(train, test, x_column, y_column, random_state=seed)\n","\n","x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=seed, stratify=y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_0w1441jR5q"},"source":["BATCH = 256\n","EPOCHS = 10000\n","PATIENCE = 10000\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./logs/weights.hdf5', monitor='val_f1_score', mode='max', save_best_only=True, verbose=1)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', mode='max', patience=PATIENCE, restore_best_weights=True, verbose=1)\n","tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', profile_batch=0)\n","\n","class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train[:,0])\n","class_weight = dict(zip(np.unique(y_train), class_weight))\n","\n","model = make_model()\n","# model.load_weights(filepath='./logs/weights.hdf5')\n","\n","model_history = model.fit(x_train, tf.keras.utils.to_categorical(y_train),\n","                          validation_data=(x_valid, tf.keras.utils.to_categorical(y_valid)),\n","                          epochs=EPOCHS, batch_size=BATCH, shuffle=True,\n","                          callbacks=[checkpoint, early_stopping, tensorboard],\n","                          class_weight=class_weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQt85sCjkEcF"},"source":["def plot_model_history(history):\n","    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","    plt.figure(figsize=(15, 3))\n","\n","    for n, metric in enumerate(['loss', 'f1_score']):\n","        plt.subplot(1, 2, n+1)\n","        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n","        plt.plot(history.epoch, history.history['val_' + metric], color=colors[0], linestyle='--', label='Val')\n","        plt.ylim([plt.ylim()[0], 1] if n > 0 else [0, plt.ylim()[1]])\n","        plt.ylabel(metric.replace('_', ' ').capitalize())\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","\n","\n","plot_model_history(model_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UO_x6pXkk8nh"},"source":["model.load_weights(filepath='./logs/weights.hdf5')\n","\n","y_predict = classifier_predict(model, x_test, threshold=0.5, network=True)\n","plot_confuncion_matrix(y_test, y_predict)"],"execution_count":null,"outputs":[]}]}