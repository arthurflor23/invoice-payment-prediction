{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict-invoice-payment.ipynb","provenance":[],"collapsed_sections":["sblNVizzgVXB","am9ZLnD3g5Ou","mnQYhrSvijYu","adcYWvKLaJfk"],"authorship_tag":"ABX9TyPdO8Evm41HXeDCcSyjflgD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"4bnYcMJAZUoe"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhtPjoHTq7BZ"},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sblNVizzgVXB"},"source":["## Modules"]},{"cell_type":"code","metadata":{"id":"Ipzzmxz2JqLN"},"source":["!pip -q install pandas-profiling imbalanced-learn tensorflow-gpu --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uqEobEKvHp8"},"source":["from pandas_profiling import ProfileReport\n","\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import RobustScaler\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","SEED = 42"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BU7vD7zggBsy"},"source":["## Data Functions"]},{"cell_type":"markdown","metadata":{"id":"am9ZLnD3g5Ou"},"source":["### Plot"]},{"cell_type":"code","metadata":{"id":"jeBrT3TrvQ6h"},"source":["def plot_countplot(df, cols=[0], title=None, rotation=0):\n","    for col in cols:\n","        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 2))\n","        g = sns.countplot(x=np.squeeze(df[col] if isinstance(col, str) else df[:,col]), ax=ax)\n","        g.set_xticklabels(labels=g.get_xticklabels(), rotation=rotation)\n","        g.set_title(title)\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confusion Matrix', report=True):\n","    if report: print(classification_report(y_test, y_predict, zero_division=True))\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    g = sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax)\n","    g.set_title(title)\n","\n","def plot_feature_importance(features, importances):\n","    features = np.array(features)\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(len(features)):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    plt.figure(figsize=(10, 8))\n","    plt.barh(range(len(features)), importances[indices])\n","    plt.yticks(range(len(features)), features[indices])\n","    plt.title('Feature Importance')\n","    plt.gca().invert_yaxis()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uK7Aw-6ahHY8"},"source":["### Data Manager"]},{"cell_type":"code","metadata":{"id":"VeB_rcoahJbR"},"source":["def setup_buckets(df, col, bins, sufix='Bucket'):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]} to {bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    df[col + sufix + 'Category'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","    df[[col + sufix]] = df[[col + sufix + 'Category']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return df\n","\n","def split_data(df, date_column, split_date, train_months=6, test_months=1, category_cols=None):\n","    split_date = pd.to_datetime(split_date)\n","    train_date = split_date - pd.DateOffset(months=train_months)\n","    test_date = split_date + pd.DateOffset(months=test_months)\n","\n","    df_ranged = df[(df[date_column] >= train_date) & (df[date_column] < test_date)].copy()\n","    df_ranged.reset_index(drop=True, inplace=True)\n","\n","    if category_cols is not None:\n","        df_ranged[category_cols] = df_ranged[category_cols].apply(lambda x: pd.Categorical(x, ordered=False).codes)\n","\n","    train = df_ranged[df_ranged[date_column] < split_date]\n","    test = df_ranged[df_ranged[date_column] >= split_date]\n","    return train, test\n","\n","def preprocess_data(train, test, x_column, y_column):\n","    x_train, y_train = train[x_column].values[::-1], train[y_column].values[::-1]\n","    x_test, y_test = test[x_column].values, test[y_column].values\n","\n","    x_train, x_test = np.vectorize(np.log)(x_train + 1), np.vectorize(np.log)(x_test + 1)\n","\n","    qt = RobustScaler(quantile_range=(25.0, 75.0))\n","    qt.fit(np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test)))\n","\n","    x_train, x_test = qt.transform(x_train), qt.transform(x_test)\n","    return x_train, y_train, x_test, y_test\n","\n","### Deprecated ###\n","def resample(df, x_column, y_column, func):\n","    dtypes = df[x_column].dtypes.to_dict()\n","    dtypes.update(df[y_column].dtypes.to_dict())    \n","    x, y = df[x_column].values, df[y_column].values\n","\n","    try:\n","        x, y = func.fit_resample(x, y)\n","        y = np.expand_dims(y, axis=1)\n","    except:\n","        pass\n","\n","    xy = np.concatenate((x, y), axis=1)\n","    data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","    data = data.astype(dtypes)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyGnVMMYhgKz"},"source":["def features_selection(train, test, x_column, y_column, target_class='macro avg', random_state=None):\n","    insertion = lambda l, x: [l[:i] + [x] + l[i:] for i in range(len(l) + 1)]\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","\n","    def _loop_selection(groups, score=-1, features=[]):\n","        scores_groups = []\n","\n","        for feature_group in groups:\n","            x_column = np.array(flatten(feature_group))\n","            x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column)\n"," \n","            clf = RandomForestClassifier(n_estimators=100, criterion='entropy', class_weight='balanced', random_state=random_state, n_jobs=-1)\n","            clf.fit(x_train, np.squeeze(y_train))\n","            y_predict = clf.predict(x_test)\n","\n","            cr = classification_report(test[y_column].values, y_predict, output_dict=True, zero_division=True)\n","            scores_groups.append(cr[target_class]['f1-score'])\n","\n","        local_score = max(scores_groups)\n","        local_features = list(groups[scores_groups.index(local_score)])\n","        print(f'Score: {local_score:.8f} >>> {local_features}')\n","\n","        if local_score > score:\n","            return local_score, local_features\n","\n","        return score, features\n","\n","    x_column = x_column.tolist()\n","    groups = np.expand_dims(x_column, axis=1).tolist()\n","\n","    score, features = _loop_selection(groups)\n","    x_column.remove(features[0])\n","\n","    for group in x_column:\n","        score, features = _loop_selection(insertion(features, group), score, features)\n","\n","    return score, features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXPTfb-fatqS"},"source":["## Read Dataset"]},{"cell_type":"code","metadata":{"id":"faoC-yeX30uK"},"source":["### Read dataset ###\n","df = pd.read_csv('InvoicedDocuments_v7_Buckets.csv', sep=';', na_values=['N/I'], parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'])\n","# df = pd.read_csv('InvoicedDocuments_v7_Default.csv', sep=';', na_values=['N/I'], parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'])\n","df.sort_values(by=['DocumentDate'], ascending=True, ignore_index=True, inplace=True)\n","\n","### First filters ###\n","df.dropna(subset=['ClearingDate', 'PaymentTerms', 'CustomerRegion'], inplace=True)\n","df = df[(df['DueDate'] > df['DocumentDate']) & (df['ClearingDate'] > df['DocumentDate'])]\n","\n","### Feature engineer ('Days to' columns) ###\n","date_int = lambda x: x.astype('timedelta64[D]').astype(int)\n","\n","df['DocumentDateToDueDate'] = date_int(df['DueDate'] - df['DocumentDate'])\n","df['DueDateToClearingDate'] = date_int(df['ClearingDate'] - df['DueDate'])\n","df['DocumentDateToClearingDate'] = date_int(df['ClearingDate'] - df['DocumentDate'])\n","\n","### Feature engineer (numeric columns) ###\n","for col in ['Invoiced', 'Paid', 'Open']:\n","# for col in ['Invoiced', 'Paid', 'Open', 'PastDue', 'PaidPast']:\n","    df[f'Ratio{col}Amount'] = df[f'{col}Amount'] / df[f'{col}Documents']\n","\n","df.fillna(0, inplace=True)\n","\n","### Feature engineer (date columns) ###\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","    df[col + 'Month'] = pd.DatetimeIndex(df[col]).month\n","    df[col + 'DayOfYear'] = pd.DatetimeIndex(df[col]).dayofyear\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ETafw3HDVMk"},"source":["df = setup_buckets(df, col='DueDateToClearingDate', bins=[1], sufix='Bin')\n","df = setup_buckets(df, col='DueDateToClearingDate', bins=[1, 4, 8], sufix='Bucket')\n","\n","plot_countplot(df, cols=['DueDateToClearingDateBinCategory', 'DueDateToClearingDateBucketCategory'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnlsCE98ayDD"},"source":["## Train and Test Data"]},{"cell_type":"code","metadata":{"id":"9XSHX4q-JCTg"},"source":["y_column = np.array(['DueDateToClearingDateBucket'])\n","x_column = np.array([\n","                    # 'AvgPastDueDays',\n","                    # 'AvgDSOPastDueDocuments',\n","                    # 'RatioPaidAmount',\n","                    # 'RatioInvoicedAmount',\n","                    # 'RatioOpenAmount',\n","                    # 'RatioPastDueAmount',\n","                    # 'RatioPaidPastAmount',\n","                    # 'DocumentAmount',\n","                    # 'DocumentDateToDueDate',\n","                    #     'CompanyKey',\n","                    #     'PaymentTerms',\n","                    #     'CorporateDivision',\n","                    #     'CustomerKey',\n","                    #     'CustomerRegion',\n","                    # 'DocumentDateDay',\n","                    # 'DueDateDay',\n","                    # 'DocumentDateWeekDay',\n","                    # 'DueDateWeekDay',\n","                    # 'DocumentDateDayOfYear',\n","                    # 'DueDateDayOfYear',\n","\n","\n","                    'PastDueDays',\n","                    'AvgPastDueDays',\n","                    'StDevPastDueDays',\n","\n","                    'DSOPastDueDocuments',\n","                    'AvgDSOPastDueDocuments',\n","                    'StDevDSOPastDueDocuments',\n","\n","                    'PaidDocuments',\n","                    'PaidDocumentsOnTime',\n","                    'PaidDocuments1to3',\n","                    'PaidDocuments4to7',\n","                    'PaidDocuments7more',\n","\n","                    'PaidAmount',\n","                    'PaidAmountOnTime',\n","                    'PaidAmount1to3',\n","                    'PaidAmount4to7',\n","                    'PaidAmount7more',\n","\n","                    'AvgPaidAmount',\n","                    'AvgPaidAmountOnTime',\n","                    'AvgPaidAmount1to3',\n","                    'AvgPaidAmount4to7',\n","                    'AvgPaidAmount7more',\n","\n","                    'StDevPaidAmount',\n","                    'StDevPaidAmountOnTime',\n","                    'StDevPaidAmount1to3',\n","                    'StDevPaidAmount4to7',\n","                    'StDevPaidAmount7more',\n","\n","                    'InvoicedDocuments',\n","                    'InvoicedAmount',\n","                    'AvgInvoicedAmount',\n","                    'StDevInvoicedAmount',\n","\n","                    'OpenDocuments',\n","                    'OpenAmount',\n","                    'AvgOpenAmount',\n","                    'StDevOpenAmount',\n","\n","                    'DocumentAmount',\n","                    'DocumentDateToDueDate',\n","\n","                    'RatioInvoicedAmount',\n","                    'RatioPaidAmount',\n","                    'RatioOpenAmount',\n","\n","                    'CompanyKey',\n","                    'PaymentTerms',\n","                    'CorporateDivision',\n","                    'CustomerKey',\n","                    'CustomerRegion',\n","\n","                    'DocumentDateDay',\n","                    'DocumentDateWeekDay',\n","                    'DocumentDateMonth',\n","                    'DocumentDateDayOfYear',\n","\n","                    'DueDateDay',\n","                    'DueDateWeekDay',\n","                    'DueDateMonth',\n","                    'DueDateDayOfYear',\n","                    ])\n","\n","\n","train, test = split_data(df, date_column='DueDate', split_date='2020-06-01', train_months=6, test_months=1,\n","                         category_cols=['CompanyKey', 'CorporateDivision', 'CustomerKey', 'CustomerRegion', 'PaymentTerms'])\n","\n","# train = resample(train, x_column, y_column, SMOTE(sampling_strategy='auto', random_state=SEED))\n","\n","# 100:   75945\n","# 1000:  380664\n","# 10000: 309053\n","# train = train[train['CustomerKey'] == 75945]\n","# test = test[test['CustomerKey'] == 75945]\n","\n","plot_countplot(train, cols=y_column, title='Train')\n","plot_countplot(test, cols=y_column, title='Test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnQYhrSvijYu"},"source":["## Data Analysis"]},{"cell_type":"code","metadata":{"id":"l-57SpRkiitT"},"source":["# x_train, y_train, x_test, y_test = prepare_data(train, test, x_column, y_column, random_state=SEED)\n","\n","# cols = np.concatenate((x_column, y_column), axis=0)\n","# df_train = np.concatenate((x_train, y_train), axis=1)\n","# df_test = np.concatenate((x_test, y_test), axis=1)\n","# df_local = np.concatenate((df_train, df_test), axis=0)\n","\n","# df_local = pd.DataFrame(df_local, columns=cols)\n","# profile = ProfileReport(df_local)\n","\n","# profile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCodPFGoaiC_"},"source":["## Random Forest"]},{"cell_type":"code","metadata":{"id":"n6iwNiRfJsIu"},"source":["x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column)\n","\n","clf = RandomForestClassifier(n_estimators=100, criterion='entropy', class_weight='balanced', random_state=SEED, n_jobs=-1)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","y_predict = clf.predict(x_test)\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMrLlQBzQaYq"},"source":["plot_feature_importance(x_column, clf.feature_importances_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lD-DkbJJQeCu"},"source":["# y_predict = (clf.predict_proba(x_test)[:,1] >= 0.9999).astype('int')\n","# plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBs5DoIHFipX"},"source":["x_column = np.array([                   \n","                    ['PastDueDays'],\n","                    ['AvgPastDueDays'],\n","                    ['StDevPastDueDays'],\n","\n","                    ['DSOPastDueDocuments'],\n","                    ['AvgDSOPastDueDocuments'],\n","                    ['StDevDSOPastDueDocuments'],\n","                    ])\n","\n","# target_class='3', \n","score, features = features_selection(train, test, x_column, y_column, random_state=SEED)\n","\n","# Final score: 0.56753507 >>> [['StDevDSOPastDueDocuments'], ['StDevPastDueDays'], ['DSOPastDueDocuments'], ['AvgPastDueDays'], ['AvgDSOPastDueDocuments']]\n","\n","print(f'\\nFinal score: {score:.8f} >>> {features}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adcYWvKLaJfk"},"source":["## Neural Network"]},{"cell_type":"code","metadata":{"id":"DNlVeilQcFCu"},"source":["# from sklearn.utils.class_weight import compute_class_weight\n","# from sklearn.model_selection import train_test_split\n","\n","# import tensorflow_addons as tfa\n","# import tensorflow as tf\n","\n","# def make_model():\n","#     model = tf.keras.models.Sequential(name='cubricks')\n","#     model.add(tf.keras.layers.Input(shape=x_train.shape[1]))\n","\n","#     model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(512, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(256, kernel_initializer='glorot_normal'))\n","#     model.add(tf.keras.layers.PReLU())\n","#     model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     # model.add(tf.keras.layers.Dense(32, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     # model.add(tf.keras.layers.Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","#     # # model.add(tf.keras.layers.Dropout(rate=0.1))\n","\n","#     # model.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2)))\n","#     # model.add(tf.keras.layers.PReLU())\n","#     # model.add(tf.keras.layers.BatchNormalization(renorm=False))\n","\n","#     model.add(tf.keras.layers.Dense(np.unique(y_train).shape[0], activation='softmax'))\n","\n","#     model.compile(\n","#         optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8, amsgrad=True),\n","#         loss=tf.keras.losses.CategoricalCrossentropy(),\n","#         # loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n","#         metrics=[tfa.metrics.F1Score(num_classes=np.unique(y_train).shape[0], average='weighted')])\n","\n","#     return model\n","\n","\n","# model = make_model()\n","# model.summary()\n","\n","# train, test = split_data_month_window(df, col='DueDate', date='2020-08-01', month_window=12)\n","# x_train, y_train, x_test, y_test = prepare_data(train, test, x_column, y_column, random_state=SEED)\n","\n","# x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=SEED, stratify=y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_0w1441jR5q"},"source":["# BATCH = 256\n","# EPOCHS = 10000\n","# PATIENCE = 10000\n","\n","# tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', profile_batch=0)\n","\n","# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./logs/weights.hdf5',\n","#                                                 monitor='val_f1_score', mode='max',\n","#                                                 save_best_only=True, save_weights_only=True, verbose=1)\n","\n","# early_stopping = tf.keras.callbacks.EarlyStopping(patience=PATIENCE,\n","#                                                   monitor='val_f1_score', mode='max',\n","#                                                   restore_best_weights=True, verbose=1)\n","\n","# class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train[:,0])\n","# class_weight = dict(zip(np.unique(y_train), class_weight))\n","\n","# model = make_model()\n","# # model.load_weights(filepath='./logs/weights.hdf5')\n","\n","# model_history = model.fit(x_train, tf.keras.utils.to_categorical(y_train),\n","#                           validation_data=(x_valid, tf.keras.utils.to_categorical(y_valid)),\n","#                           epochs=EPOCHS, batch_size=BATCH, shuffle=True,\n","#                           callbacks=[checkpoint, early_stopping, tensorboard],\n","#                           class_weight=class_weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQt85sCjkEcF"},"source":["# def plot_model_history(history):\n","#     colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","#     plt.figure(figsize=(15, 3))\n","\n","#     for n, metric in enumerate(['loss', 'f1_score']):\n","#         plt.subplot(1, 2, n+1)\n","#         plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n","#         plt.plot(history.epoch, history.history['val_' + metric], color=colors[0], linestyle='--', label='Val')\n","#         plt.ylim([plt.ylim()[0], 1] if n > 0 else [0, plt.ylim()[1]])\n","#         plt.ylabel(metric.replace('_', ' ').capitalize())\n","#         plt.xlabel('Epoch')\n","#         plt.legend()\n","\n","\n","# plot_model_history(model_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UO_x6pXkk8nh"},"source":["# model.load_weights(filepath='./logs/weights.hdf5')\n","\n","# y_predict = classifier_predict(model, x_test, threshold=0.5, network=True)\n","# plot_confuncion_matrix(y_test, y_predict)"],"execution_count":null,"outputs":[]}]}