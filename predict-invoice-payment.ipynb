{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZEqyNjfabZKZbAeVCcrtD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Sr5D501BGk9-"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-jyFOsoxIMQ"},"source":["from google.colab import drive\n","\n","drive.mount('./gdrive', force_remount=True)\n","%cd './gdrive/My Drive/Colab Notebooks/cubricks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Px0shqnqxJFf"},"source":["!pip -q install imbalanced-learn xgboost tensorflow-gpu pandas-profiling --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDaPtZ4YxJfB"},"source":["from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from xgboost import XGBClassifier\n","\n","from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n","from pandas_profiling import ProfileReport\n","\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","SEED = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-Qnt9bzx2Q7"},"source":["def plot_countplot(df, cols=[0], title=None, rotation=0):\n","    for col in cols:\n","        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 2))\n","        g = sns.countplot(x=np.squeeze(df[col] if isinstance(col, str) else df[:,col]), ax=ax)\n","        g.set_xticklabels(labels=g.get_xticklabels(), rotation=rotation)\n","        g.set_title(title)\n","\n","def plot_confuncion_matrix(y_test, predict, title='Confusion Matrix', report=True):\n","    if report: print(classification_report(y_test, y_predict, zero_division=True))\n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n","    g = sns.heatmap(confusion_matrix(y_test, predict), fmt='d', square=True, annot=True, cmap='Blues', ax=ax)\n","    g.set_title(title)\n","\n","def plot_feature_importance(features, importances):\n","    features = np.array(features)\n","    indices = np.argsort(importances)[::-1]\n","    print(f'Feature ranking:')\n","\n","    for f in range(len(features)):\n","        print(f'{importances[indices[f]]}\\t{features[indices[f]]}')\n","\n","    plt.figure(figsize=(10, 8))\n","    plt.barh(range(len(features)), importances[indices])\n","    plt.yticks(range(len(features)), features[indices])\n","    plt.title('Feature Importance')\n","    plt.gca().invert_yaxis()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"az4Gw8Dxx4JM"},"source":["def setup_buckets(df, col, bins, sufix='Bkt'):\n","    bins = [-np.inf] + bins + [np.inf]\n","    labels = [f'{bins[i]} to {bins[i+1]-1}' for i in range(len(bins[:-1]))]\n","    df[f'{col}{sufix}Label'] = pd.cut(df[col], bins=bins, labels=labels, right=False, include_lowest=True)\n","    df[[f'{col}{sufix}']] = df[[f'{col}{sufix}Label']].apply(lambda x: pd.Categorical(x, ordered=True).codes)\n","    return df, labels\n","\n","def split_data(df, date_column, split_date, train_months=6, test_months=1):\n","    split_date = pd.to_datetime(split_date)\n","    train_date = split_date - pd.DateOffset(months=train_months)\n","    test_date = split_date + pd.DateOffset(months=test_months)\n","\n","    df_ranged = df[(df[date_column] >= train_date) & (df[date_column] < test_date)].copy()\n","    df_ranged.reset_index(drop=True, inplace=True)\n","\n","    train = df_ranged[df_ranged[date_column] < split_date]\n","    test = df_ranged[df_ranged[date_column] >= split_date]\n","    return train, test\n","\n","def resample(df, x_column, y_column, func):\n","    dtypes = df[x_column].dtypes.to_dict()\n","    dtypes.update(df[y_column].dtypes.to_dict())    \n","    x, y = df[x_column].values, df[y_column].values\n","\n","    try:\n","        x, y = func.fit_resample(x, y)\n","        y = np.expand_dims(y, axis=1)\n","    except:\n","        pass\n","\n","    xy = np.concatenate((x, y), axis=1)\n","    data = pd.DataFrame(xy, columns=np.concatenate((x_column, y_column)))\n","    data = data.astype(dtypes)\n","    return data\n","\n","def preprocess_data(train, test, x_column, y_column, nominal_columns=None):\n","    if nominal_columns is not None:\n","        train, test = train.copy(), test.copy()\n","        to_number = lambda x: [int(w) if str(w).isnumeric() else int(''.join(format(ord(c), '') for c in str(w).lower())) for w in x]\n","\n","        for col in nominal_columns:\n","            idmax = train[col].value_counts().idxmax()\n","            train[col] = train[col].fillna(idmax)\n","            test[col] = test[col].fillna(idmax)\n","\n","        train[nominal_columns] = train[nominal_columns].apply(lambda x: to_number(x))\n","        test[nominal_columns] = test[nominal_columns].apply(lambda x: to_number(x))\n","\n","    x_train, y_train = train[x_column].values, train[y_column].values\n","    x_test, y_test = test[x_column].values, test[y_column].values\n","\n","    qt = RobustScaler(quantile_range=(25.0, 75.0))\n","    qt.fit(x_train, y_train)\n","\n","    x_train = qt.transform(x_train)\n","    x_test = qt.transform(x_test)\n","\n","    return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AsCc4LrMx5a2"},"source":["def feature_selection(train, test, x_column, y_column, nominal_columns=None, target_class='macro avg', random_state=None):\n","    insertion = lambda l, x: [l[:i] + [x] + l[i:] for i in range(len(l) + 1)]\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    x_column = x_column.tolist()\n","\n","    def _loop_selection(groups, score=-1, features=[]):\n","        scores_groups = []\n","\n","        for feature_group in groups:\n","            x_column = np.array(flatten(feature_group))\n","            x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column, nominal_columns)\n"," \n","            clf = RandomForestClassifier(256, criterion='entropy', class_weight='balanced', random_state=random_state, n_jobs=-1)\n","            clf.fit(x_train, np.squeeze(y_train))\n","            y_predict = clf.predict(x_test)\n","\n","            cr = classification_report(test[y_column].values, y_predict, output_dict=True, zero_division=True)\n","            scores_groups.append(cr[target_class]['f1-score'])\n","\n","        local_score = max(scores_groups)\n","        local_features = list(groups[scores_groups.index(local_score)])\n","        print(f'Score: {local_score:.8f} >>> {local_features}')\n","\n","        if local_score > score:\n","            return local_score, local_features\n","\n","        return score, features\n","\n","    in_score, in_features = _loop_selection([[flatten(x_column)]])\n","    score, features = _loop_selection(np.expand_dims(x_column, axis=1).tolist())\n","    x_column.remove(features[0])\n","\n","    for group in x_column:\n","        score, features = _loop_selection(insertion(features, group), score, features)\n","\n","    if in_score > score:\n","        return in_score, in_features\n","\n","    return score, features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NptR4CUnx6O6"},"source":["### Read dataset ###\n","df = pd.read_csv('InvoicedDocuments_full.csv', sep=';', parse_dates=['DocumentDate', 'DueDate', 'ClearingDate'], low_memory=False)\n","df.sort_values(by=['DueDate'], ascending=False, ignore_index=False, inplace=True)\n","\n","### First filters ###\n","df.dropna(subset=['ClearingDate'], inplace=True)\n","df = df[(df['DueDate'] > df['DocumentDate']) & (df['ClearingDate'] > df['DocumentDate'])]\n","\n","### 'Days to' columns ###\n","date_int = lambda x: x.astype('timedelta64[D]').astype(int)\n","df['DueDateToClearingDate'] = date_int(df['ClearingDate'] - df['DueDate'])\n","\n","### Date columns ###\n","for col in ['DocumentDate', 'DueDate']:\n","    df[col + 'Day'] = pd.DatetimeIndex(df[col]).day\n","    df[col + 'WeekDay'] = pd.DatetimeIndex(df[col]).weekday\n","\n","### Ratio columns ###\n","later_columns = ['TotalLatePaidInvoices', 'TotalPendingLateInvoices', 'SumAmountLatePaidInvoices', 'SumAmountPendingLateInvoices']\n","total_columns = ['TotalPaidInvoices', 'TotalPendingInvoices', 'SumAmountPaidInvoices', 'SumAmountPendingInvoices']\n","ratio_columns = ['RatioLatePaidInvoices', 'RatioPendingLateInvoices', 'RatioAmountPaidLateInvoices', 'RatioAmountPendingLateInvoices']\n","\n","for l, t, r in zip(later_columns, total_columns, ratio_columns):\n","    df[r] = (df[l] / df[t]).fillna(0)\n","\n","### Fix customer columns ###\n","for col in ['CustomerShipToKey', 'CustomerSoldToKey']:\n","    df[col] = df[col].fillna(df['CustomerKey'])\n","\n","df.fillna(0, inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTxSr3o858IY"},"source":["df, bin_class_names = setup_buckets(df, col='DueDateToClearingDate', bins=[4], sufix='Bin')\n","df, bucket_class_names = setup_buckets(df, col='DueDateToClearingDate', bins=[1, 4, 8], sufix='Bkt')\n","\n","plot_countplot(df, cols=['DueDateToClearingDateBinLabel', 'DueDateToClearingDateBktLabel'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IaYnlq5g5-TN"},"source":["columns = {\n","    'target': [\n","        'DueDateToClearingDateBin',\n","    ],\n","    'numeric': [\n","        'AvgDaysLatePaidInvoices',\n","        'StdevDaysLatePaidInvoices',\n","        'AvgDaysLatePendingInvoices',\n","        'StdevDaysLatePendingInvoices',\n","        'RatioLatePaidInvoices',\n","        'TotalLatePaidInvoices',\n","        'TotalPaidInvoices',\n","        'RatioAmountPaidLateInvoices',\n","        'SumAmountLatePaidInvoices',\n","        'SumAmountPaidInvoices',\n","        'RatioPendingLateInvoices',\n","        'TotalPendingLateInvoices',\n","        'TotalPendingInvoices',\n","        'RatioAmountPendingLateInvoices',\n","        'SumAmountPendingLateInvoices',\n","        'SumAmountPendingInvoices',\n","        'InvoiceAmount',\n","        'PaymentTerm',\n","        'PaymentFrequency',\n","    ],\n","    'nominal': [\n","        'CompanyKey',\n","        'CorporateDivision',\n","        'CustomerShipToKey',\n","        'CustomerSoldToKey',\n","        'CustomerKey',\n","    ],\n","    'date': [\n","        'DocumentDateDay',\n","        'DocumentDateWeekDay',\n","        'DueDateDay',\n","        'DueDateWeekDay',\n","    ],\n","}\n","\n","y_column = np.array(columns['target'])\n","x_column = np.array(columns['numeric'] + columns['nominal'] + columns['date'])\n","\n","train, test = split_data(df, date_column='DueDate', split_date='2020-08-01', train_months=6, test_months=1)\n","\n","# train = resample(train, x_column, y_column, SMOTE(sampling_strategy='auto', random_state=SEED))\n","# train = resample(train, x_column, y_column, ADASYN(sampling_strategy='auto', random_state=SEED))\n","# train = resample(train, x_column, y_column, BorderlineSMOTE(sampling_strategy='auto', kind='borderline-1', random_state=SEED))\n","# train = resample(train, x_column, y_column, BorderlineSMOTE(sampling_strategy='auto', kind='borderline-2', random_state=SEED))\n","\n","plot_countplot(train, cols=y_column, title='Train')\n","plot_countplot(test, cols=y_column, title='Test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXVAX1Xi7Ep_"},"source":["x_train, y_train, x_test, y_test = preprocess_data(train, test, x_column, y_column, nominal_columns=columns['nominal'] + columns['date'])\n","\n","clf = RandomForestClassifier(n_estimators=256, criterion='entropy', min_samples_leaf=1, class_weight='balanced', random_state=SEED, n_jobs=-1)\n","clf.fit(x_train, np.squeeze(y_train))\n","\n","y_predict = clf.predict(x_test)\n","plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNfIvI2exEgH"},"source":["# test[(test[y_column[0]] == 1) & (y_predict == 0)].head(50)\n","\n","# y_predict = (clf.predict_proba(x_test)[:,1] >= 0.9999).astype('int')\n","# plot_confuncion_matrix(test[y_column].values, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9o9IjsA18upj"},"source":["# plot_feature_importance(x_column, clf.feature_importances_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_iuG5pdZ0HB"},"source":["x_column = np.array([\n","    # ['StdevDaysLatePaidInvoices',\n","    # 'AvgDaysLatePaidInvoices',\n","    # 'StdevDaysLatePendingInvoices',\n","    # 'AvgDaysLatePendingInvoices'],\n","\n","\n","    ['TotalLatePaidInvoices'],\n","    ['RatioLatePaidInvoices'],\n","\n","    ['SumAmountLatePaidInvoices'],\n","    ['RatioAmountPaidLateInvoices'],\n","\n","    # ['TotalPendingLateInvoices'],\n","    # ['RatioPendingLateInvoices'],\n","\n","    # ['SumAmountPendingLateInvoices'],\n","    # ['RatioAmountPendingLateInvoices'],\n","\n","\n","    # ['PaymentFrequency',\n","    # 'PaymentTerm',\n","    # 'InvoiceAmount'],\n","\n","    # ['DocumentDateWeekDay',\n","    # 'DueDateWeekDay',\n","    # 'DocumentDateDay',\n","    # 'DueDateDay'],\n","])\n","\n","\n","score, features = feature_selection(train, test, x_column, y_column, nominal_columns=columns['nominal'] + columns['date'], random_state=SEED)\n","\n","print(f'\\nFinal score: {score:.8f} >>> {features}')"],"execution_count":null,"outputs":[]}]}